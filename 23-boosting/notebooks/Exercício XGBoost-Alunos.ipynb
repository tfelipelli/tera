{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução e o Dataset   \n",
    "   O dataset utilizado neste exercício foi o dataset Previsão de Segurança do Motorista, disponibilizado pela Porto Seguro. Há um desbalanceamento na classe de saída e vamos descobrir como lidar com esse tipo de dado.\n",
    "    Em um primeiro momento, devemos fazer a análise exploratória dos dados.\n",
    "    \n",
    "#### About this Dataset\n",
    "\n",
    "#### Data Description\n",
    "\n",
    "In this competition, you will predict the probability that an auto insurance policy holder files a claim.\n",
    "\n",
    "In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder.\n",
    "\n",
    "#### File descriptions\n",
    "- train.csv contains the training data, where each row corresponds to a policy holder, and the target columns signifies that a claim was filed.\n",
    "- test.csv contains the test data.\n",
    "- sample_submission.csv is submission file showing the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../input/train.csv', na_values=-1)\n",
    "test=pd.read_csv('../input/test.csv', na_values=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = train.drop(['id','target'], axis=1).values\n",
    "targets = train.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que podemos ver?\n",
    "\n",
    "- As classes de previsão estão muito desbalanceadas\n",
    "- Alto viés para o 0 (se chutar tudo 0 pode acertar abstante)\n",
    "- Devemos ter alguma ponderação para balancear as classes na hora de treinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que:\n",
    "- ps_calc_* não estão relacionadas com a classe desejada\n",
    "- Podemos removê-las para evitar curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return 'gini', gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train = train.drop(unwanted, axis=1)  \n",
    "test = test.drop(unwanted, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como lidar com classes desbalanceadas?\n",
    "\n",
    "Stratified Kfold !\n",
    "\n",
    "Stratified KFold é utilizada para manter a distribuição de cada label consistente entre cada batch de treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 5\n",
    "skf = StratifiedKFold(n_splits=kfold, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More parameters has to be tuned. Good luck :)\n",
    "params = {\n",
    "    'min_child_weight': 10.0,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 7,\n",
    "    'max_delta_step': 1.8,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'subsample': 0.8,\n",
    "    'eta': 0.025,\n",
    "    'gamma': 0.65,\n",
    "    'num_boost_round' : 700\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id', 'target'], axis=1).values\n",
    "y = train.target.values\n",
    "test_id = test.id.values\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['target'] = np.zeros_like(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    # Convert our data into XGBoost format\n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid)\n",
    "    d_test = xgb.DMatrix(test.values)\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n",
    "    # and the custom metric (maximize=True tells xgb that higher metric is better)\n",
    "    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "\n",
    "    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n",
    "    # Predict on our test data\n",
    "    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n",
    "    sub['target'] += p_test/kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (somostera)",
   "language": "python",
   "name": "somostera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
