{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos baseados em ávore são modelos não lineares, não paramétricos bem conhecidos dentre os modelos de aprendizado surpervisionado. Além disso, eles facilitam a interpretação, tem uma predição rápida (ao contrário do kNN). Por fim, eles resolvem bem tanto problemas de classificação quanto de regressão.\n",
    "\n",
    "![title](../data/imgs/boosting0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Revisando conceitos\n",
    "\n",
    "Árvores de dcisão são um tipo de algoritmo de aprendizado surpervisionado que funciona tanto para varáveis categóricas quanto contítunas. Nessa tecnica, nós dividimos a população (ou a amostra) em sub grupos homongêneos baseado no `split` que nos dê o maior ganho de informação com base em cada uma das variáveis de entrada.\n",
    "\n",
    "![title](../data/imgs/boosting1.png)\n",
    "\n",
    "Fun fact: exemplo de uma árvore de decisão\n",
    "http://pt.akinator.com/\n",
    "\n",
    "![title](../data/imgs/boosting2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árvores de decisao podem ser usadas tanto para classificação quanto para regressão. No caso da regressão, o valor obtido por uma folha no dado de treino é basicamente a média das observações que cairam naquela região. Portanto, se um dado não visto cair nessa região, faremos sua previsão com base nos valores treinados.\n",
    "\n",
    "No caso de classificação, o valor da folha é a moda das categorias.\n",
    "\n",
    "Nos dois casos, o processo de divisão (splitting) da ávore ocorre na ávore toda até algum critério de parada (por exemplo, número máximo de splits ou profundidade da árvore) sejam atingidos. Contudo, caso não haja `prunning`, ou limitar alguns parâmetros, é normal que ocorra `overfitting` dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relembrando hiperparâmetros\n",
    "\n",
    "![title](../data/imgs/boosting3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O dataset\n",
    "O dataset foi originalmente tirado do National Institute of Diabetes and Digestive and Kidney Diseases. Seu objetivo é diagnosticar se um paciente têm ou não diabetes, baseado em certas medidas de diagnóstico incluídas no dataset. Em particular, todos os paciantes aqui são mulheres de pelo menos 21 anos de idade.\n",
    "\n",
    "Vamos prever se conseguimos descobrir se os pacientes têm diabetes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv(url, names=names)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70779220779220775"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Library\n",
    "#Import other necessary libraries like pandas, numpy...\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Create tree object \n",
    "model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  \n",
    "# model = tree.DecisionTreeRegressor() for regression\n",
    "# Train the model using the training sets and check score\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "y = array[:,8]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=7, stratify=y, test_size=0.2)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O problema das Decision Trees é que elas tendem ao `overfit` e isso ocorre ainda pode ocorrer com prunning.\n",
    "\n",
    "![title](../data/imgs/boosting4.png)\n",
    "\n",
    "A ideia é que ao invés de usarmos uma árvore única, combinarmos árvores mais \"fracas\"/menores para construir noss modelo?\n",
    "\n",
    "![title](../data/imgs/boosting5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The literary meaning of word ‘ensemble’ is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree based models.\n",
    "\n",
    "Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, ‘how much on an average are the predicted values different from the actual value.’ Variance means, ‘how different will the predictions of the model be at the same point if different samples are taken from the same population’.\n",
    "\n",
    "You build a small tree and you will get a model with low variance and high bias. How do you manage to balance the trade off between bias and variance ?\n",
    "\n",
    "Normally, as you increase the complexity of your model, you will see a reduction in prediction error due to lower bias in the model. As you continue to make your model more complex, you end up over-fitting your model and your model will start suffering from high variance.\n",
    "\n",
    "A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. Ensemble learning is one way to execute this trade off analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "É aí que entra o Ensemble. Métodos de ensemble são métodos que envolvem agrupar modelos preditivos para atingir uma melhor estabilidade ao combinar classificadores mais \"fracos\". \n",
    "Fun fact: O time ganhador do [Netflix prize](https://www.youtube.com/watch?v=ImpV70uLxyw&t=192s) aplicou cerca de 800 algoritmos combinados !\n",
    "\n",
    "Assim como vimos em `kNN`, modelos de árvore também podem sofrer de excesso de bias (muito genérico/pouco treinado) e variance (muito complexo, tende ao overfit). Técnicas de Ensemble foram desenvolvidas para combater esse trade-off de bias vs variance.\n",
    "\n",
    "![title](../data/imgs/boosting6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a technique used to reduce the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same data set. The following figure will make it clearer:\n",
    "\n",
    "Como é possível ver na imagem acima, Bagging é uma tecnica usada para reduzir a variância das nossas predições em um dataset ao combinar o resultados de multiplos classificadores baseados em diversas amostras (com repetição) do mesmo dataset.\n",
    "\n",
    "![title](../data/imgs/boosting7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Multiple DataSets:\n",
    "Sampling is done with replacement on the original data and new datasets are formed.\n",
    "The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model\n",
    "Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting\n",
    "Build Multiple Classifiers:\n",
    "Classifiers are built on each data set.\n",
    "Generally the same classifier is modeled on each data set and predictions are made.\n",
    "Combine Classifiers:\n",
    "The predictions of all the classifiers are combined using a mean, median or mode value depending on the problem at hand.\n",
    "The combined values are generally more robust than a single model.\n",
    "\n",
    "- Criar os multiplos datasets:\n",
    "\n",
    "  - Sampling with replacement do dataset original, formando datasets novos\n",
    "  - O número de features por dataset e o número de dados por dataset viram **hiperparâmetros**\n",
    "- Treine diversos modelos:\n",
    "\n",
    "  - Um para cada dataset (Pela imagem acima, cada Model é treinado para um draw)\n",
    "  - Geralmente é usado o mesmo tipo de classificador\n",
    "  \n",
    "- Combine os classificadores:\n",
    "  - As previsões/combinação de todos os classificadores é feita por meio da média, mediana ou moda, dependendo do problema trabalhado\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77922077922077926"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "cart = tree.DecisionTreeClassifier(criterion='gini')\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../data/imgs/boosting8.png)\n",
    "\n",
    "## Mas e as Florestas Aleatórias\n",
    "\n",
    "Florestas Aleatórias (ou Random Forests) são uma extensão de Bagging aplicado a Decision Trees.\n",
    "\n",
    "Nela, os samples são treinados no dataset, mas além disso, as árvores são construídas de forma que os classificadores tenham sua correlação reduzida entre eles. Em outras palavras, ao invés de escolher a melhor divisão considerando todos os atributos disponíveis. Nós escolhemos o melhor com base em um subset de features (todos os subsets têm o mesmo tamanho), selecionados de forma aleatória.\n",
    "\n",
    "![title](../data/imgs/boosting9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75974025974025972"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vantagens e Desvantagens da Random Forest\n",
    "\n",
    "- Consegue lidar com datasets grandes e com muita dimensionalidade\n",
    "- Os modelos conseguem definir a importância de cada variável !\n",
    "- É capaz de lidar com dados faltantes\n",
    "\n",
    "\n",
    "- Limitações para problemas de regressão: não consegue extrapolar dados para além do que foi treinado\n",
    "\n",
    "![title](../data/imgs/boosting10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E para lidar com os casos de alto bias?\n",
    "\n",
    "### Boosting !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pela definição, o termo `Boosting` se refere à família de algoritmos que convertem um conjunto de `weak` learners e um único `strong` learner.\n",
    "\n",
    "\n",
    "Pegando o exemplo anterior, suponha que queremos trabalhar com o problema de SPAM. Isto é queremos classificar se um email é SPAM ou não.\n",
    "\n",
    "Sendo assim, como que classificaríamos emails como sendo de SPAM?\n",
    "\n",
    "- Email que têm apenas uma imagem (de promoção) é SPAM\n",
    "- Email que tem apenas links, é SPAM\n",
    "- Email cujo corpo consiste de uma sentença como \"Você ganhou na mega-sena\", é SPAM\n",
    "- Email do somostera, não é spam\n",
    "- Email de uma fonte conhecida, não é spam\n",
    "\n",
    "Definimos uma série de regras para classificar um email como `spam` ou não. Mas você acha que essas regras individualmente são fortes o bastante para classificar um email? Não.\n",
    "\n",
    "Para combinar esses `weak learner` em um `strong learner` vamos utilizar métodos como:\n",
    "- Usar uma média ponderada\n",
    "- Considerar com base nas features, o `majoroty vote`\n",
    "Por exemplo, se um dado não visto cair em 3 dos 5 critérios acima, ele será considerado como SPAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mas e como isso funciona?\n",
    "\n",
    "Beleza, boosting combina uma série de weak learners de forma a formar um modelo só. Mas como que boosting identifica essas \"regras fracas\"?\n",
    "\n",
    "No caso de árvores, técnicas de boosting se baseiam no que é chamado de \"Stump tree\", ou seja, \"árvores com apenas uma divisão\". A cada iteração que o algoritmo é aplicado, cria-se uma nova divisão com base nos erros do passado (o modelo \"presta mais atenção\" nos erros que tivemos). Sendo assim, após várias iterações, essas \"weak rules\" são combinadas, formando um modelo final.\n",
    "\n",
    "Sendo assim, pode-se dizer que Boosting tem uma atenção maior em modelos classificados incorretamente.\n",
    "\n",
    "\n",
    "![title](../data/imgs/boosting11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75324675324675328"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É válido saber que os modelos de Boosting têm uma série de hiperparâmetros que não vimos aqui. Na segunda metade da aula vamos explorar outra biblioteca, que implementa o Gradient Boosting de uma forma mais eficiente que o Sklearn e que têm sido bastante famosa para ganhar várias competições de Data Science: o XgBoost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (somostera)",
   "language": "python",
   "name": "somostera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
