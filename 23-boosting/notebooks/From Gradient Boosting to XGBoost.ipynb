{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../data/imgs/boosting13.png)\n",
    "\n",
    "Como vimos, Gradient Boosted Decision Trees é uma tecnica de Boosting que utiliza Stumped Decision Trees para a melhora gradual do modelo.\n",
    "\n",
    "Mas como que isso é feito?\n",
    "\n",
    "![title](../data/imgs/boosting12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse ciclo é repetido de formas sucessivas enquanto construímos novos modelos e fazemos a devida combinação com nosso modelo de `Ensemble`. Nós iniciamos o ciclo ao calculuar os erros para cada valor do nosso dataset de treino. Então, nós construímos um novo modelo para fazer a previsão em cima desses erros e adicionamos essa previsão-de-erro ao nosso \"ensemble de modelo\".\n",
    "\n",
    "Para fazer a previsão, nós adicionamos as predições de todos os modelos anteriores. Essas previsões são usadas para calcular novos modelos e então, as acoplamos no nosso ensemble.\n",
    "\n",
    "Confuso? Para entender um pouco melhor a ideia, vamos relembrar a ideia de Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relembrando Regressão Linear\n",
    "\n",
    "Na aula de Regressão Linear, vocês viram um método analítico (dos mínimos quadrados). Ele funciona bem com datasets pequeno, mas temos alguns problemas como datasets maiores. Aqui, vamos apresentar um método mais computacional que funciona com datasets mais largos\n",
    "\n",
    "Na regressão, quando queremos definir a curva que define nosso modelo, nós podemos fazê-lo por meio do seguinte algoritmo:\n",
    "\n",
    "- Ter um modelo $F_1(x)$ que faça a predição dos nossos dados\n",
    "- Medir os nossos resíduos (erros): $h(x) = y - F(x)$\n",
    "- Criar um novo modelo: $F_2(x) = F_1(x) + h_1(x)$\n",
    "\n",
    "Podemos repetir isso $M$ vezes de forma a irmos combinando cada vez mais modelos:\n",
    "![title](../data/imgs/boosting14.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Só que nós queremos minimizar os erros. Podemos fazer isso definindo uma função de custo que define o nosso erro, por exemplo: \n",
    "\n",
    "$f(x) = \\sum_{i=1}^N{L(y_i,h(x_i))}$, ou seja, todos os nossos dados de treinamento. Aqui, então, $L$ pode ser o erro quadrático $L = (y_i - h(x_i))$\n",
    "\n",
    "E para minimizar nós introduzimos a ideia de gradiente:\n",
    "\n",
    "Para um dado ponto x do nosso dataset, queremos reduzí-lo ao mínimo local:\n",
    "\n",
    "![title](../data/imgs/boosting15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, queremos fazer o nossa previsão ter seu erro cada vez mais reduzido, de forma que ele seja **muito próximo** de 0.\n",
    "\n",
    "Matemátiquês a parte, o que está acontecendo é basicamente isso:\n",
    "\n",
    "$x_i = x_{i-1} - \\eta\\frac{d_f(x_{i-1})}{dx}$, em que $\\eta$ representa a _intensidade_ que queremos fazer esse ajuste em função do erro\n",
    "\n",
    "![](../data/imgs/gradient_descent_example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferença do GD para Gradient Boost é que no GD vamos atualizando ponto a ponto, por interação. No caso do Gradient Boosting, nós não temos uma forma clara da forma da nossa função: ela é apenas um amontoado de árvore, em que cada árvore é atualizada pr interação. No caso do Gradient Descent, após a iteração _i_, você têm a atualização do ponto _i_ analizado. Para árvores, após a _i_ ésima interação, teremos _i_ árvores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Dataset\n",
    "\n",
    "Vamos dar uma explorada no nosso dataset\n",
    "\n",
    "O que ele quer dizer para a gente?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "data = pd.read_csv('../data/housing/inputs/train.csv')\n",
    "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "\n",
    "my_imputer = Imputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "my_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(train_X, train_y, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 18916.565732\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "predictions = my_model.predict(test_X)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81451278660507664"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção de Hiperparâmetros\n",
    "\n",
    "O XGBoost têm alguns hiperparâmetros que podem afetar muito a acurácia do seu modelo e velocidade de treinamento.\n",
    "\n",
    "Os dois primeiros são:\n",
    " - n_estimators \n",
    " - early_stopping_rounds\n",
    "\n",
    "#### Número de estimadores\n",
    "\n",
    "Número de estimadores podem resultar em `underfitting` enquanto números elevados podem levar a `overfitting`. Novamente, caímos aqui no dilema entre complexidade e generalização de treinamento. Você deve experimentar com o dataset para achar os valores ideiais. Um número mágico (para ajudar) variam de 100 a 1000 mas depende muito do __learning rate__, que já vamos falar.\n",
    "\n",
    "#### Early stopping rounds\n",
    "\n",
    "Esse argumento, por sua vez, fornece uma forma de encontrar o valor ideal automaticamente. Ele faz com que o modelo pare de iteragir quando o score de validação seja concluido, mesmo que o número de estimadores não tenha atingido o número ideal. Aqui, é ideal usar um valor alto para o número de estimadores e, então, usar o early_stopping_rounds para encontrar um tempo ótimo para parar de iterar. Como por aleatoriedade, o val score pode não melhoreas, queremos um número de rounds seguidos em que não há melhora para considerarmos que nosso modelo deva parar.\n",
    "Um valor bom aqui é val_score = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
    "             eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning_rate\n",
    "\n",
    "Aqui é exatamente o **$\\eta$** que vimos lá em cima quando falamos do Gradient Descent. E, como ele determina a intensidade, ele é uma das peças chaves para atingir os melhores modelos do XGBoost\n",
    "\n",
    "Na prática, ele serve para controlar o _overfitting_ do nosso modelo.\n",
    "\n",
    "Assim, não precisamos nos preocupar muito com um número máximo de árvores. Se usarmos early_stopping, o número apropriado de árvores vai ser encontrado de forma automática, evitando o overfit.\n",
    "\n",
    "Sendo assim, é melhor um learning rate alto ou baixo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
    "             eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81114743909258136"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_jobs\n",
    "\n",
    "Em datasets grandes, em que o tempo de execução é levado em consideração, você pode fazer uso de paralelism para que seus modelos sejam executados mais rapidamente. É comum setar o número de jobs (n_jobs) exatamente igual ao número de cores na sua máquina. Em datasets pequenos, contudo, isso não faz muita diferença\n",
    "\n",
    "O XGBoost tem vários outros parâmetros que valem a pena serem discutidos, mas para um primeiro momento, esses são os ideais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### objective_functions\n",
    "\n",
    "Objective Functions (ou Loss Functions) são as funções que vimos nos exemplos. Elas quantificam o quão longe nossa predição está do resultado real\n",
    "\n",
    "Em outras palavras, queremos, minimizar essa função\n",
    "\n",
    "No xgboost elas recebem os seguintes nomes:\n",
    " - reg:linear para problemas de regressão\n",
    " - reg:logistic usados em problemas de classificação quando você só quer a classe, não a chance de ser ela\n",
    " - reg: binary usados em problemas de classificacao quando você quer a chance de pertencer a cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81114743909258136"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 72x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADLCAYAAABgQVj0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XtcFWX+B/DPcEBAxIpFRTRFMUVY\ny01JK9M089KWZqXdrN01TTfUyvJXlltmL7uZhhEqi7lmtV4q8daWLBmVqyFmXlFTFFDkZl4AhQMc\nnt8f4wznwOFwLjPzzJnzfb9evA5nLs/zZebMlznPzDyPwBgDIYQQ4/LjHQAhhBB1UaInhBCDo0RP\nCCEGR4meEEIMjhI9IYQYHCV6QggxOEr0hBBicJToCSHE4CjRE0KIwfnzDuAqejyXEEJcJzizEJ3R\nE5/0/vvvIywsDIIgQBAEBAQEYPbs2bzDIkQVgk76utFFEMTYsrKyMGLECFy6dMnhcvHx8ZgwYQIl\nfuINnDqjp0RPfEJAQABqa2tdWic4OBhVVVUqRUSIIqjphpCIiAgAcDnJA5CTvMlkUjQmQrRGiZ4Y\nVlpaGoqLiz0ux2KxYNCgQQpERAgf1HRDDGnkyJHYtm2bomX6+/ujrq5O0TIJ8RC10RNCiMFRGz3x\nTRMmTFCtbGqvJ96IzuiJ4VgsFlUTcmFhITp16qRa+YS4gM7oie8ZNWqU6mfdnTt3VrV8QpRGiZ4Y\nym233aZ6HWazWfU6CFESNd0Q4ga1m4cIcRI13RAi2b17N86dOwcAWL58eZP5NTU1GDx4sPz+oYce\nQklJSbPlrVq1SvEYCVELJXpiGGfOnGl23tq1axEeHm6TvJctWyYn/1atWuGLL76Q53355Zfo27dv\ns+Xt2rVLgYgJ0QYlemIY11xzjcP527dvR79+/TBnzhwAwNy5cxEeHg4AOHfuHDp06GCzvKOHo1qq\nixA9oTZ6QgDk5OQgNjZWfn/kyBH07t272eVbmk+IRujJWOJ7/ve//+H222/nHQYhWqGLscT3DB8+\nXPU65s2bp3odhCiJEj0xlMLCQtXruHLliup1EKIkarohhnPbbbdh586dqpSdnp6OESNGqFI2IW6g\nNnpClHb27FlERkbyDoMQCbXRE9+lxu2PCQkJlOSJV6JETwzp0qVLmDVrlmLlnThxAomJiYqVR4iW\nqOmGGNpTTz2Fjz/+2KMysrOzER8fr1BEhCjKqaYbf7WjIISnAwc+hslkgsVicWv92NhY5OTkKBwV\nIdqiphtiWGfPAtnZkJO8v7/z5zXdu3cHAOT40SFCvB99iolh7d5t+17qu6Zz584YNGiQzeDhWVlZ\neOihhxAQEAAAOHnypDhj/35AH82bhLiN2uiJIfXoAZw4oVBh3boBp04pVBghiqLbK4lvio5WMMkD\nYpLPzlawQEK0RYmeGMrmzUBurgoFZ2SoUCgh2qCmG2Iop08D11+vUuHh4cDVgUoI0QnqAoH4looK\nIDSUdxSEaIra6InvCAnRKMkHBWlQCSHKokRPvJ7JBFy+rFFl1dVA+/YaVUaIMqjphhB3VFfT2T3R\nA2q6IcZ3xx2erV9VVeXeisOGeVYxIRqiRE+81tixwE8/ib/X1dXhwIEDiI6OdrjOl19+CUEQT4Lq\n6+vRrVs3fPTRRzh16hSsv90uX75cXm7r1q1YtWoVxowZ01DQzp3AF18AgM1yr7/+ulJ/HiGKoURP\nvJKfH7BpU8P7gIAA3HjjjSgpKZGn/fe//22yXp8+feTfT1x9qmr69Ono3r07BEFAQkKCPF9K4Pn5\n+Zg/fz6aNHPW1zdZLiQkxLM/jBAVUBs98Tqffw48/rjttIyMDHlg8IKCAnTp0gWlpaVo3749ysrK\n0K5du2bLq6urkzs8k9ZtLC8vDzk5ObjnnntsZ9xyS5NOdc6cOYPOnTu78ZcR4jK6j54YU2Eh0KlT\nw/tHHnkE//znP9G2bVvV6oyKisLIkSORkpKiWh2EuIESPTGekSMBq04n9aFdO6CsjHcUxDdRoifG\nEhkp9jGvS0FB4i2XhGiLbq8kxrFtm46TPCAmeasLwYToCSV6onubNolNNronOHVyRYjmKNET3YuN\n5R2Bk9q3B959F3jmGd6REGKD2uiJbgUEAHFxwL59vCNxQUAAUFdHww8SrVAbPdGXNWvWoFevXhAE\nAYIgICAgAM8991zTB5EA/PqrmC/37we+/ZZDsJ5atqzJpD//+c/y3y4IArp3746cnBwOwRFfQ2f0\nRFUXLlxAWFiY3WRu7Z577kF0dDSSkpIANDR3T58OXJ3kPUJCgCtXAMbwxhtvoEOHDpg2bVqzi1dV\nVSE0NFQevJwQF9DtlYSfsrIyzJw5E2vWrHFpvcrKSgwfPhxZWT97devHXYKA79z4A1544QUsWrRI\nhYiIQVGiJ3zcf//92Lhxo0dlhIWF4fz58wpFpK3AwECYzWa319+8eTMGDhyI9tTvPWkZJXqivZb6\nlXGFNyb72NhYRdrdKyoqUFBQgLi4OAWiIgZGiZ5oa/Xq1XjyyScVLTMkJASXNRs+yjN+fn6ov9qj\nJSEaobtuiLaGDBmieJnekuRff/11VZL8DTfcoHiZxPdQoieK6NevH7p27apK2d4wmMcbb7yhSrnH\njx9HeHi4KmUT30FNN0QRO3fuxG233aZa+SkpKZg6dapq5XtCEIQWbx/1RHFxMSIiIlQrn3g1aqMn\n2vjkk0/wl7/8RdU6OnXqhMLCQlXrcFdJSQk6dOjAOwzimyjRE20odacJad6YMWOwefNm3mEQ/aGL\nsUQbn3zyiSb1bLIeJFYnpHFn1bZ161ZN6iHGRImeeCw+Pt7u9H/961/y7/bamFNTU23er1ixwmE9\n8+fPdyM6dTkT0zONerO0WCzygOLOmj17tkvLE2KNEj1RzcGDBwHY3pGybNkynDt3DgAwZcoUm4Q3\nefJkdOzYsdnytDp7doUzMS1duhQAYDabERoaCpPJBMYYAgICbJZ79dVXcfHiRbtl0G2WxBOU6Inq\nUlNTUXZ1TNW5c+fa3C745ptv2izrqGMvNQf/dldoaKjD+ffeey86d+6Mzp07Y+HChXjllVcAAAEB\nAaitrbVZdsGCBcjMzLRbTnl5uSLxEt9EF2OJx2pra5ucnTqjpqYGrVq1kt8fOXIEvXv3bnb5efPm\nYd68ee6EqJovvvgC48ePd2vd3NxcREdHo7KyEm3atHG4bExMDI4ePepWPcTQ6K4boo1Zs2Zh8eLF\nqtdTV1cHf39/1evRI2/qCoJoiu66Idr4+eefVa/j3Xff9dkkD1DTDfEMndETr9C1a1fk5+fzDsOu\nESNGID09nXcYxDfRGT3RTmBgoGpl19TU6DbJA8CECRNULb9z586qlk+MjxI9UYQnA220ZOXKlaqV\nrYTJkye7fF+8sw4fPowzZ86oUjbxHZToiSISEsT+2JW2Y8cOh+Ot6gVjDB9//LHi5aanpwNDhype\nLvEt1EZPPPL888D//R8gPefUsWNHFBUVKVL23//+dyxbtkyRsrTS+JZRTwQHB6OqqqphQtu2AF2U\nJbbo9kqirptvBvbubTr9pptuwv79+z0q+8qVK2jdurVHZfBw+PBhhIWFOXzC1xmhoaGoqKhoOqO6\nGti3Dxg40KPyiWHQxViinj/9yX6SB4D9+/d7dFY7bdo0r72VMi4uDhERER59q5kxY4b9JA8AQUFi\nkjeZ3C6f+B5K9MQl3buLr7/+6ni5mpoaAOKgHI66NbA2btw4WCwWLF++XLHmDx4EQUDHjh2RkZGB\nQYMGOb3ejTfeCABISkpqeWGLRXxt4YlaQgBK9MQF8fHAyZOurcMYg7+/P3r37o34+Hhs27ZNnnfq\n1CnMmDEDgiCgpqYGaWlpMBnoTHX48OHYsWMHfvrpJwiCgKSkJFRWVsrzt2/fjtatW+Ppp58GABw4\ncMD1SiorgfXrAX00wRKdojZ60qKbbgI8bHJvKjMTuPNOhQv1DmVlQLt2KhQ8cSLw2WcqFEx0jNro\nieeiolRI8kQdn30mfuX62994R0J0hhI9aVZMDJCXxzsK4pLu3YF//Qu49VbekRAdoURPmpg1S3yl\nXnG92K5d4mtICN84iC545z1sRDUmU8MNHcQALl8GZs8GHntMvCeW+CRK9AQAcOUKkJhISd6QFi4U\nX3fsEG+dUrEDOqJP1HRD8OabgCAAV0e5I0Y1aJCY5K+9lnckRGOU6H1cejrwj38AwcG8IyGauXgR\naN+edxREQ5TofVhsLDBiBO8oCBelpeLrgw/yjYNoghK9jwoMBHJyGt6vWLECRUVFWLRokc1ya9as\nsXn/5ZdfYs6cOQCA+vp6REREQHrozvrhu4iICFRXVwMAtm7diqioKOy16hyn+moXCVu3bsU111yD\n3377Tbk/zgtERUUBgLy9R40ahczMTCxYsMBmuVtuuUXu92bBggU4ePBgk7LWrFlj0x9+//795aEH\nW3wg8quvxNurnnnGZvKwYcOwZcsWrF69GgBU62+faIMSvY957TWgvh5oPE7IxYsXcf78eSQnJwMA\nCgoKAACzZ8+2Wa5Pnz646aabAAAnTpwA0NCvjfQqCQoKAgDk5+djxowZiIuLa5h3tS+b/Px8lJeX\no2fPnrD40JXgI0eO4IknnpC398SJE3HnnXfiH//4h81y0dHRCA0NBSD2jGlvJK/G+yg/Px9t27ZF\ncXGxcwk6JgZYuhRYsgT45hu53j59+iAtLQ2A2A8R8V7UBYIPOXpUPKZbcvjwYcTFxaG0tBTt27eH\n2WxucajAgoICdOnSRX5tbNWqVYiMjMTevXvx8ssvN+kCwZk6jKJxFwjS9pb6sndmW0j7RnptzGw2\nY+fOnfjb3/6GPFefegsKErtDvurYsWPo1auXa2UQrVB/9KTBnDnAW2+Jd9c0lp2djby8PIwfP161\n+ocOHYrq6mrskh7k8fG+bp54YhS+/fZb1eo4dOgQRo8ejdOnT7tXwPTpYnNObKyygRGlUaInog4d\ngJIS3lE04uOJXpVOzdTQqxdw+DDgpeMD+ACnEj3tPYPz8xPb5Alxy7Fj4mt4OHDuHN9YiNvoYqyB\ndelCSZ4o5Nw54LrreEdB3ESJ3qBmzQKu3jhDiDIuXBBf6VZLr0OJ3oC++w5YvJh3FMQePz/xoVSv\nzpWMiYMUvPAC70iIk+hirIH8+CPQty/Qti3vSFogZbkffgAGD+Ybi8aOHwd69hR/18eh56F33gEG\nDACGDuUdia+iEaZ8ydtvizlT90nemo8leQC44Qbx9bvv+MahmJdfFpP8gAGaVrtr1y4MHz4cgiDg\nuuuuQ3R0NAIDAyEIAiZPnozz589rGo/e0Rm9AVRXi8+4eI2QELFfZH189jTn7w/U1fGOQgVPPSV2\ngRodrUrxc+fOxaeffopDhw7JTws7UlNTg8DAQOzcuRO3GnfELbqP3hd06gQUFvKOwg133AH89BPv\nKDSxbNkyzJgxA0OHDsXw4cMRFRWFyspKZGVlITU1FePGjcO6desQEBDAO1RldOsG5OaKFyQUEB0d\njdzcXI/KqK+vh7+/P+qNdxuac1d7GGN6+DE0s9nMBgwYwK677jq2YcMGVlVVJc87cOAAu/vuu5mf\nnx+rqKhwqdwtW5SOVB0ZGRkMAEtISGB79+6Vp589e5YlJSWxiIgI9swzz3CMUHm7du1igYGBrLy8\n3Ol1nnjiCTZu3DgVo9LYv//t0epjx45lRUVFCgUjqqioYAcOHFC0TM6cyrG8E7yhE33r1q3dWm/u\n3LksLy/P7rzkZMb27GFMEDyJTH2vvvqqW8k7Ly+PBQUFqRCRNm6//XaWm5vrcTnevA1snDnD2IQJ\nLq/2+++/qxBMAwOdWDiVY6npRgX9+/dHeno6wsLCPConNDRU7qJWIt2wUlAAXH+9R8WrZubMmfjw\nww89KqO6uhrff/89Ro8erVBU6hMEoeVugV1QVVWFiooKu52WeR0nByMuLCxEaWkp/qTB+LZ9+/bF\nvn37VK9HZXTXDQ9DhgzBnj17PE7yAFBRUYHHHnsMtbW1AID58xvmde3qcfGKW7JkCQoKCjxO8oDY\nxfHo0aPhp1A7r5oYY3j22WcVTfIAEBwcjPbt2+Ohhx5StFwuLBZg926gqkp838yFpZCQEE2SPADs\n27cPjzzyiCZ18UZn9Ar57rvvEB8fj7Yq3d9oMpkgCBZ8/TUwcqQqVXikTZs2qKysVKXsqVOnIiUl\nRZWyPVVWVoZ2GvRQJnVJbAivvy6etQiCTR8dBw8eRJ8+fTQPp7munr0EndFr6a677lItyQOAxWLB\nRx8t12WSDwoKUi3JA0BKSgqWLFmiWvnuqq+v1+x+7dLSUvTt21eTulQnfTVlTP5q+uCDD3JJ8oD4\nTzTIq+5Pdh2d0XuIMYazZ8+iU6dOmtSnt3bFjh07oqioSJO6qqqqEKyjUcwtFgtMJpOmdbZt21Ye\nJtDrlZUB48djUGkpdliPa8lJ9+7dcfLkSd5huIruo9fCqFHqDiChZ5WVlWjTpo2mdeol0bVr1w5l\nZWW8wyAKqq+v94prQo1Q043aYmJiuCT5a6+9VvM67Xn44Yc1r7O8vBzFxcWa12vNYrFwTfJ/+MMf\nuNWtND0lVj8/P6xfv553GKrQz1b2QkePHuVS78WLF5GZmcmlbklQUBC+/vprLnUfkwbD4OTVV1/l\nWv/vv//u8ZOielBWVqa7J1UjIiJ4h6AKSvRueu2117jW/9JLL3Gt/4LUNzkHQ4YM4Vb3pk2b8M47\n73CrXxIfH887BI/p8U6XwYMHIzk5mXcYiqNE76b+/ftzrT8rKwvp6elc6j5z5gz3i6ICpw7dv/nm\nGy71NmaE3hl1cn2wib179/IOQXF0MdYNJSUl6NChA+8wFH8S01l+fn7cv3KfPHkS3bt35xoDb4wx\nbv/wiG7QxVi13HzzzbxDACA2I/CwYMECLvVa6969O9fmIz24//77eYfgtt9++413CA6lpaXxDkFR\ndEbvhieffBKrV6/mHYbP69KlCwo0HBh3w4YNeOCBBzSrryVKdN/Li95j18O3VifRGb1aZs2axTsE\nbnjf8WLtzJkzmtant4t0M2bM4B2C2/T+YJJebmFWCiV6Nzh6FL2rnd7GUlNT7S77yiuvNJm2du1a\nm/fLly93MTp1ff755w7nv/322wAaLpZu3LixyTLr1q0D0HDnzrZt2+Dn54dffvkFANC7d2/Mnj3b\nZll7JkyY4GL0ntm+fXuLy7S0fXbt2oWBAwfaTBMEASNGjAAA3HffffL06upqh2U9+uijLcajVy3d\nNSR9jmpqappdxmKxwHK1R0zpcyJtM2k9e5+jlrYrAIwZM6bFZbwJJXqF2WtKmDJlipz4unbtinPn\nzjVZRpr//vvvy9NKSkpUitJ9LXWsVddojLxx48bBbDYDAMxmM0JDQ5s8aHX48GHU19ejX79+AIDs\n7GwsXLgQgOOHsvR4e97jjz9u837mzJkAgEOHDmHbtm249dZb8fPPP9tcRE1MTESPHj0AADt37pSn\n+/v74+LFi83WpUVnampp6aGvxp+jKVOmwGw2y9+mQ0NDYTKZ5C4opM9J4z5r7H2O/P390bFjR4/i\n8zaU6DXy5ptvAgAuX76M8PDwJvOlayV79uzBqVOnsH37dvTr1w9z5szRNM6WuPOVNjAwEPn5+Vi4\ncKHdbzFTp06VD9jVq1fL3SpcunTJYbmOkiBP1tdvrly5AgB4/vnnMdKqR7qkpCQA4j+GZ599FkuX\nLgVge6Lg7+/v8ME4vf79znA1dpPJhMDAQHzwwQe4dOmSzeeopc9J43++/v7+Tf6ReBqf7jk7QonK\nP17l5MmTLi1vNpsdzrdYLPLvSoxOpKasrCyX1zl+/LjN+8bbo76+3q1YgoOD3VrPXf379/e4DLPZ\nzH777bcm03NycuTfpSElrYectGflypUex8OLmHpc0/hz5K6WtitjjF1//fWK1KUBGmFKLS+88AIW\nLVrEOwyfFxISgsuXL2tWX0pKCqZOnapZfS0ZOHAgfv75Z95huCU8PNxuE6Ze8HpGxQ10141a9HL3\nhXTxUmuffvopl3oby87O1rQ+PSV5oGk7tjf56quveIfgkNSUZhR0Ru+GrKwsDBgwgHcYCA4ORpU0\nNJuG9HC2o7e+6Xm4ePGi4W4DJC6jM3q1DBgwAMePH+cdBt577z0u9WZkZHCp1xqvLigSExO51GsP\nJXl1vPXWW7xDUBwlejdJt83xMn36dG4PzNx1111c6rXGq/sDPQx6AgC33nor7xA8FhoayjsEu06f\nPs07BMVRoncT714MefebzXPAiAsXLmg+hJ/ktddew9atW7nUbe25557jHYLH9LAdG/v999+xbNky\n3mEojhK9BwICArjUKwgC5s6dy6VuSX19PbcLVj/99BOXeiVxcXFc6xcEgcvoXkobMmQIAgMDeYdh\n44033uAdgiroYqyHUlNTMWXKFN5hcPHjjz9i8ODBmtZpMpnkx9550sMFaSPw0nFa9YQuxmpB6qNE\nK61atdK0PkcGDx7c5KlDtekhyQP8Bs3w9/fnUq9a/Pz85P5oeOPVHKgFSvQe6tKlKz74QJvuTG+5\n5RaHnTzx8Pnnn6Nt27aq13P69GlUVFSoXo8rjhw5omnCb+XEo/veaOHChdzvqw8ODtbNSYQaKNF7\n4MIFYPdu4Pnn/fDggw+qWldkZCR2796tah3uKi8vR0hIiGrl79u3DyEhIbq7S6N3796aPTzm7++P\nmro6YPRoTerT2sCBA7kmWh7Po2iJ2ujd9L//Abff3nT65s2bFe3iNDs7GxUVFRg2bJhiZaolJiYG\nR48eVbTMnj176n40on379iEoKAgxMTGqlN+mTRtUVlY2TIiIAIqLVanL12RmZmLIkCEQDh8G/vhH\n3uG4w7mxJJ3tFEflH6+SmsqYo3644EaHTfZMmzZNkXK0VFZWxk6cOKFIWUOHDlWkHK3ExsYqXuZ7\n771nf0ZgoOJ16UFiYqLbndy5avny5Q111dYytmGDJvUqzKkcyzvBe2Wid0ZlZSWbMWOGW+uWlpay\n5557TuGItBUWFuZ2T5w3hoWx4uJihSPShlL/5J944gl28OBBxwu9+64idemNvd49lXbo0CHV69AI\nJXqljR7t3nqLFy9mANhbb71ld35GRgbr0aMHmz9/vgfR6UNUVNNpI0aMYOPHj2f5+fl211m8eDEz\nmUzsl19+aZh4tatebzVu3Dg2ffp0l9Y5ePCg6/8oWrVybXkvEhAQoEq5kyZNcrzAX/6iSr0qoUSv\npAEDFC5QoTM/vSktVagglQ5yHqKioli3bt1Yenq6zdgDx48fZxMmTGAAWGJiovsVhIYqEKU+lZeX\ns6SkJEXKiomJYWfPnnVu4bFjFalTA07lWLoY64Q//hE4dEjhQgUB0Me2V0zHjkBREe8o9K+sDFB8\nFMDwcEDH/bsroW3btnjrrbcwffp0p9cZM2YMTCYT0tLSXK9w2DDAiXGCOXPqYiwl+hbExgI5OSoU\nbMBEr7jHHwdaGGzbG6mS6AGfuhtn3Lhx2LhxI8aNG4f4+HiEhYXh9OnT2LFjB3744Qe8/PLL8gDj\nHrnzTsDBcI46QIneU+fOiSdKqjBYop8yBUhNVbjQpCSAUw+dalIt0QPAyJHAtm0qFa5fqh5O1dVA\no0HHdYQSvScGDwZ+/FHFCgyW6OvqAFWezu/aFcjPV6FgflRN9ADQpg1gfd+9D1D9cHrkEWDtWhUr\ncBv1deOuCRNUTvIGExSkUpIHAB12Zat7lZUq7hAftXYtMGcO7yjcRom+kSVLgPXreUfhPSorxW+2\nqunTBzh2TMUKDEq1r1g+7O23AXcu6uoANd1YycwUr71owiBNN61aAar3s2awi4yqN91Ya9dOrNDg\nNDucamuBkyeBXr00qMwp1HTjqn79eEfgfTTpTLO4WDzAiOvKygCzmXcUxhEQIF4D8TKU6K/asgXQ\nWeeIuvf88xpWNnashpUZTGAgoFEvmz6hUyfgo494R+ESSvQAxo8H7ruPdxTex5O2+cWLF9ud3uzw\nhP/5D5Cb636FBnHy5Em70ydNmuR4xfHjfaIJxx3NbbszZ840v9L06cBtt6kUkfJ8vo2e251oXt5G\nn5cHREVpXKlBnv7UtI3e2jffGLY/e26Hk7+/eOGbH2qjb0n//g1J3vofXkJCAmJiYpCRkdFknZUr\nVwIAcnNzIQgN27hDhw4wm83yq2TKlCnycjU1NXjhhRds5uXm5uLmm28GAJvy9E56jik2NhaLFi0C\nIG63W265BRMnTrTZBgBw7NgxrFq1CoB4pvTYY48BADZt2mTzGhERIa+zdOlSPPzwww0DUjRK8oIg\neOW2A8R4a2tr0alTJ3m7Nf7sAMCjjz4q/z558mS5b/6JEyfKr2az2ebvb9++PY4cOQIAtqNyjR4t\nNjtc5c2fP3vWrl2LESNGYOzYsUhISADQ8LlqLMrqLEUQBGRkZKCsrEx+BYDly5dj/vz5Nsu98847\ntgXV1QF//rPNJEEQMGDAAADioDx64LOJPisL2LOn4b0gCLjnnnsAAO+//z6OHj2K/Px8DB8+3Kny\nSkpKEBwcLL/ak52dLSdFiclkQteuXQGIB6i32LJFfJ00aRJGjRoFAEhOTsbMmTPx2WefITo6usk6\nQVefLrxy5QoAwGw2Y+zYsVixYoX8ao0xhnXr1tkOnzdunM0y3rjtAHEbTJo0CYGBgfJ2c/TZkdaR\nfPbZZ/Jr423NGEPv3r1RXFzcdFSuwkLg2mvlt976+bNn5syZSE9Px969e5GcnIyCggK7nyug4bMo\nyc/PR0hIiPxqz5EjR/Dyyy83nfH118Dx4zaTpNHg1Bx5zSXO9n6m8o/u1dbWerT+nj17GGOMXbhw\nQZzgxb1XLlnS8Pu+fftUrSsyMpIxxlhKSoo44fJlVevTQks9fF66dMnjOsaPH88YY2zhwoX2F8jO\n9rgOPQEa/mZ7lBjfYfHixYwxptnAKE6i3iub07o1YHVyBACoq6uDv8oPmNTW1iIgIEB848Vt9N26\nAadOcQwgJgZQeMhCLXFro7e2YwcwaBDnIJSji8MpIIDHbcDURm/P8OFNkzwA1ZM8gIYk7+W4JnkA\nWLaMcwAGMGiQTXs9UUBtLfA3JxvPAAAJLElEQVTQQ7yjsMunEv3EiYCd66vEBQ6akLUzdChw6RLv\nKLxfYSEQFsY7CmP58kvg73/nHUUTPpHow8PFa3h2rsnwIQjibVledobv7y924qcLPXt6ZV8uggBE\nRupo158/D1y4IAbmpQICxB8/vWSz998HHnsM6NCBdyQyvWwaVf3+O7Bxo9g2rwubNvG+99YtFguw\napUKo2256vx5oLRUDOibbzgH45offxR3Pff2ZInJ5PVn9XV1YqtJc8/aae6aa4A1a8TPqE74RKKX\n1NfzjuCqMWPE18REvnG4oU8fcWhFrsLCGpKTvkf/aeKOO8TXTz7hG4fMYvHKb0bWfv1VfJ02jW8c\nMh2exPnEXTd8Loa3QBe3CbjGZBLzgm60agX07g3s3887EpfoctdHRYn/hby0TxxdblNtEo93jTAV\nFxeHefPm8Y5DNn78+GbnffHFFx6X3/bMGZR37uxxOcnJychs5qxWiTiV8vHHH+Pbb7+1Oy8hIQF3\natY/dMvU3vdKcbTvveV4Umrf19UJ8Pf3PJd5y763Op6cSvS6+c7Wvn17hxtZT/QUZ3JycrPz9BRn\ncwlJoqdYHdFTnI72PR1PytNTnC0dT435VBs9IYT4Ikr0hBBicJToCSHE4HSb6J93MHzRNDv3Ua1Z\nswYbN25EQUEB1qxZA0DsUrjWwVVvqYe5qqoqbN261a04v/vuu2bnLV26FOfPn28SpxSfpKKiQu5h\n79SpUyi1uv+28XspZlfV1tbiP//5j9155eXleOmll2ymZWVlIS8vT475U6u7MVJSUgDApifOqqoq\nrFmzBps3b7b7N7rC1X0vdUVrva2kfS/FJcXTuPfQJUuWuB2nq/v+7bffln+X4qisrLTZL9WNRnMp\nLCwEAI8urDra9wCa7HvrOF588UV52nvvvSf/br3d1q9fb7Muj30PNGyrxse9dMxI3WRLFixY4Hac\njvZ9eXl5k31vzXrb7d+/Xx7gRNrm1p8TSXMD9TjN2d7PVP5hd955p22XbM307rhy5com05599ln5\n99TUVJt5Tz/9NGOMsTlz5thMnzFjBlu2bBlLS0uzW4+zDh8+7PSyZrOZMcZYYGAgY4yxnTt3yvPO\nnj0r/15VVWWznvReitnakCFDnK5/xYoVdqe3atWqybRvv/2WFRYW2tRvNptZZWUlY6zp/tm9ezdj\njLHz588zxhgrLi62mf/MM880G1fjec3t++ZYL2+97Z5++mk5LmmbW7v77rsZY4zde++9LtUncWXf\nSwCwo0ePyu8HDhxoM1/afpK//vWvNutac7TvGx9Pze17e8dT4zgeeOAB9v333zPGGAsJCWG9evVi\njDVsN+u4rI9FZyi57623lXTcS8dMv379GGOMzZs3jzHG2IsvvsgYY2zt2rUu1SdxtO/tHU8S68+c\nlA8k1tvc+u+yt02stptTOVa3Z/QAEBkZiYSEBLnv7Z49e+LgwYMAxDNI6Sfx6oNHaWlpmDx5MpKS\nkgAATz31FLZt22a37A8//BAAMGHCBACedziWkJCAHj164OLFi+jfv7883TrOVq1aARD/c2/ZsgVx\ncXHyco8++ii+//57AE37yg4KCoLFYpFj9jTOe++9FwCQk5MDAHJ/71KcdXV1mD9/Pjpd7fQqKCgI\ngiAgMDAQISEhEAQBubm5iIyMlMuNj49HSkoKrrvuOgAN29VdqampTfa9xHqbNiZtK2nfx8fHA2g4\nW5K+pQDAp59+ig0bNuDy5csexSrt+8LCwmb3PSCeaQYFBaFXr17Iy8vD119/jbFjxzoc8GPfvn3y\n78zDW6EjIyOb7PvGx1OdnYd9NmzYgNdffx0AcPnyZdTV1TW73RITE+Xjz13W+/7ixYtO73tpW1kf\n99Ixc//99wNoOFteuHAhNm3ahIcfftijWO3t+7q6umbjtP7MBQYGAgDmzp1rs4z0OZHU1tYiNjbW\nozh1neiLioqQnJwsj9JSb/Vo66BBg+QfyR1XHzvs27cvAPFeU6mZoTnSVzzrkY3ckZycjD179mD9\n+vXo0aOHPH3kyJE2caampgIA7rvvPrnHzLS0NGRmZmLo0KGor6+3GZVGem8ymTyKTxITE4OtW7fi\n1KlTTT48UpyrV6/GV199ZZNYwsLCMHLkSPn9DTfcgLNnz9qsL42+BQA//PCDR3FKA3I4u++lZaRt\nZb3vpW0O2I4s1KFDBwiCgO3bt3sUq7TvO3fu7HDfT5o0CZ9cfSQ2KioKPXv2REFBARhjOHbsmN2y\n09PTAQDnFBhCsaioyKl931jr1q2xbt06AOKxdeLECYfbTTr+3GW979evX+/UvgcatpW9415qtvn3\nv/8tTxs7diyKioo8ijUmJsapfS+x/sxJx1NUozE5rT8ngPiP1tNtyrvJptmmG0l+fr78e3V1td1l\nSkpK5N9zcnLk3y87OUjF8ePHnVrOkby8PMYYY+Xl5Q6Xc+frviOuNN24or6+ntXV1THGbJtDpOYc\nxhg7ceIEY4yxgoICm3Wl5h1rrjTdSJzZ982x3vfSNrdYLPK0iooKm8+KJ6R9767GzTUS63jtcaXp\nxl3Wg2xIzXE//PCD3WXPnz9vcyw6o6V9X15e7tS+t95WzR33NTU1jDFx3/PS+DPX3L5viatNN7p5\nYKo5Xbp0kX+Xvuo0Zj0EWu/eveXfWzvZi5n1f2J3ScOxNRm6rRFPv4JpRRAE+VuE9ddI6+YaaQi7\n66+/3mZdpYZPc2bfN8d630vb3M+qe8M2bdrYfFY8Ie17d0nNXY356aA7xsbjIgPA4MGD7S7b3N/h\nDmnft3Q8Say3VXPHvdQ826ZNGw+jc1/jz5yS28wR/p8kQgghqqJETwghBkeJnhBCDE43bfSZmZkO\nbzPTGnNwK5ue4hw2bFiz8/QUJ9B8J1wmk0lXsRph33vL8UT73n2OOrVrTDfdFPMOgBBCvJBXdVOs\nr3+VhBBiINRGTwghBkeJnhBCDI4SPSGEGBwlekIIMThK9IQQYnCU6AkhxOAo0RNCiMFRoieEEIOj\nRE8IIQZHiZ4QQgyOEj0hhBgcJXpCCDE4SvSEEGJwlOgJIcTgKNETQojBUaInhBCDo0RPCCEGR4me\nEEIMjhI9IYQYHCV6QggxOEr0hBBicJToCSHE4CjRE0KIwf0/avqgPZiTx10AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(1,1))\n",
    "xgb.plot_tree(my_model, num_trees=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.8.2-py2.py3-none-any.whl\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.8.2\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (somostera)",
   "language": "python",
   "name": "somostera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
