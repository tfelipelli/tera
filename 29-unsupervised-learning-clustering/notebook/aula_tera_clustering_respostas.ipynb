{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERA - Aula 27\n",
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivos gerais de algoritmos de clustering:\n",
    "- Análise exploratória dos dados\n",
    "- Encontrar padrões e estruturas\n",
    "- Agrupar dados de forma a criar representações sumarizadas (sumarização de dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "\n",
    "- [Exemplo inicial](#Exemplo-Inicial)\n",
    "- [K-Means](#K-Means)\n",
    " - [Case K-Means Elo7](#Case-Cluster-Usuários-Elo7)\n",
    "- [DBSCAN](#DBSCAN)\n",
    "- [Case Elo7 - Cluster Frete](#Case-Elo7---Clustering-de-Frete)\n",
    "- [Hierarchical Clustering](#Hierarchical-Clustering)\n",
    " - [Exercício Prático](#Exercício-prático-Hierarchical-Clustering)\n",
    "- [Comparação Métodos Clustering](#Comparação-métodos-clustering:)\n",
    "- [Case Elo7 - Motivos de Compra](#Case-Elo7---Motivos-de-Compra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo Inicial\n",
    "Análise exploratória do comportamento dos usuários do Elo7.\n",
    "\n",
    "Dataset:\n",
    "- `tempo` (float): Tempo em segundos que um usuário permanece no site.\n",
    "- `ticket` (float): Valor gasto em reais no site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports usados no curso\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pasta contendo os dados:\n",
    "ROOT_FOLDER = os.path.realpath('..')\n",
    "DATASET_FOLDER = os.path.join(ROOT_FOLDER,'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leitura dos dados\n",
    "df_user_elo7 = pd.read_csv(os.path.join(DATASET_FOLDER, 'user_patterns_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_user_elo7.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos plotar o gráfico\n",
    "df_user_elo7.plot.scatter(x='tempo',y='ticket', alpha=0.5)\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Ticket (R$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise Exploratória\n",
    "- Média\n",
    "- Covariância\n",
    "- Tendência (Regressão Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Valor médio\n",
    "user_elo7_mean = df_user_elo7.mean().values\n",
    "\n",
    "# Covariância\n",
    "user_elo7_cov = np.cov(df_user_elo7.values[:,0], df_user_elo7.values[:,1])\n",
    "\n",
    "# Tendência\n",
    "a, b = np.polyfit(df_user_elo7.values[:,0], df_user_elo7.values[:,1], deg=1)\n",
    "f = lambda x: a*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Função auxiliar para plotar a elipse de confiança ###\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def get_confidence_ellipse(x, y, nstd=2):\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    w, h = 2 * nstd * np.sqrt(vals)\n",
    "    ell = Ellipse(xy=(np.mean(x), np.mean(y)),\n",
    "                  width=w, height=h,\n",
    "                  angle=theta, color='red', \n",
    "                  fill=False)\n",
    "    return ell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos plotar os dados no gráfico\n",
    "df_user_elo7.plot.scatter(x='tempo',y='ticket', alpha=0.5)\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Ticket (R$)')\n",
    "\n",
    "# Média\n",
    "plt.plot(user_elo7_mean[0], user_elo7_mean[1], '*r', markersize=20)\n",
    "\n",
    "# 2 desvios padrão\n",
    "ell = get_confidence_ellipse(x=df_user_elo7.values[:,0],\n",
    "                             y=df_user_elo7.values[:,1])\n",
    "ax = plt.gca()\n",
    "ax.add_patch(ell)\n",
    "\n",
    "# Tendência\n",
    "x = np.array([min(df_user_elo7.values[:,0]),max(df_user_elo7.values[:,0])])\n",
    "plt.plot(x, f(x), '--g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há algo estranho nessa análise?\n",
    "- A análise está matematicamente correta, mas talvez não seja completa;\n",
    "- Precisamos levar em consideração possíveis grupos diferentes de usuários dentro dos dados. Quantos grupos você vê? Talvez entre 2 e 4 clusters?\n",
    "\n",
    "Vamos voltar para esse problema em breve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## K-Means\n",
    "### Exemplo prático Clustering\n",
    "\n",
    "Vamos para um problema clássico.\n",
    "Utilizaremos o dataset Iris. O dataset contém os seguintes atributos:\n",
    "\n",
    "- `sepal_length`: Comprimento da sépala da flor\n",
    "- `sepal_width`: Largura da sépala\n",
    "- `petal_length`: Comprimento da pétala\n",
    "- `petal_width`: Largura da pétala\n",
    "- `species`: Espécie da flor\n",
    "\n",
    "![iris](https://cdn-images-1.medium.com/max/1600/1*1q79O5DCx_XNrAARXSFzpg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importe o dataset\n",
    "df_iris = pd.read_csv(os.path.join(DATASET_FOLDER,'iris_dataset.csv'))\n",
    "\n",
    "df_iris.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como temos um vetor de features de 4 dimensões, não faz sentido tentarmos visualizar todas as dimensões em uma só figura. Entretanto, podemos visualizar a relação entre cada uma de suas features através de um mapa de dispersão pareado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot pareado utilizando o Seaborn\n",
    "sns.pairplot(df_iris, hue=\"species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber que há uma boa separação de clusters utilizando as features `petal_length` e `petal_width`. Podemos utilizar apenas essas dimensões para a nossa análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Separe o dataset utilizando apenas as features `petal_length` e `petal_width` do dataframe\n",
    "# Coloque essas colunas em um numpy array (dica: utilize o atributo df[...].values)\n",
    "X = df_iris[['petal_length','petal_width']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot dos dados\n",
    "plt.scatter(x=X[:,0],y=X[:,1])\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós já sabemos que existem três espécies no dataset, mas podemos tentar analisar se conseguiríamos encontrar esses clusters de maneira automática. Vamos utilizar o famoso algoritmo [**K-Means**](https://en.wikipedia.org/wiki/K-means_clustering) para achar esses clusters. Verifique no site do [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) mais detalhes de implementação do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Importe o módulo do KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Crie uma instância do K-Means pelo sklearn\n",
    "kmeans = KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Treine e aplique o modelo KMeans para o dataset X\n",
    "results = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crie uma coluna no dataframe df para incluir os resultados\n",
    "df_iris['cluster'] = results\n",
    "\n",
    "df_iris.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plote o mapa de dispersão (scatter plot) dos clusters gerados\n",
    "# Veja o exemplo das moedas para se inspirar\n",
    "sns.swarmplot(data=df_iris, x='petal_length', y='petal_width', hue='cluster')\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os clusters formados fazem sentido? Veja novamente o gráfico da separação das espécies para ver se os clusters têm relação com elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.swarmplot(data=df_iris, x='petal_length', y='petal_width', hue=\"species\")\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('petal_width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos também utilizar a matriz de tabulação cruzada para verificar a relação dos clusters com as espécies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apresente a matriz de cross-tabulation (Veja o exemplo das moedas)\n",
    "ct = pd.crosstab(df_iris['cluster'], df_iris['species'])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quais são as conclusões? O K-Means funcionou do jeito que era esperado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escolha do número de clusters\n",
    "\n",
    "Nós temos diversos métodos para escolher o número ideal de clusters. Alguns deles estão resumidos neste [artigo](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method). O método mais utilizado, entretanto, é o método do \"cotovelo\" (*elbow method*). \n",
    "\n",
    "Mas, antes de falarmos do método do cotovelo, nós precisamos definir o que é um bom cluster. É claro que isso depende de cada caso, mas as seguintes características são desejadas para a maioria dos clusters:\n",
    "- Dados não muito dispersos -> Inércia\n",
    "- Dados dentro dos clusters possuem perfil semelhante\n",
    "- Quantidade aproximadamente uniforme de dados em cada cluster (controverso)\n",
    "\n",
    "#### Inércia\n",
    "\n",
    "A inércia de um cluster é definida como a soma das distâncias quadráticas de cada ponto de um cluster ao seu respectivo centroide, somada através de todos os clusters. Quanto maior é a inércia, maior será a dispersão dos clusters. Portanto, desejamos escolher um número de clusters que nos possibilite ter uma inércia baixa. Simples, mas temos um problema... O mínimo valor de inércia que podemos obter é quando cada ponto do nosso dataset pertence ao seu próprio cluster. Portanto, precisamos escolher um balanço entre baixa inércia e baixo número de clusters. \n",
    "\n",
    "Para isso, utilizamos o gráfico de cotovelo. O eixo horizontal do gráfico representa o número de clusters utilizados e o eixo vertical representa a inércia total dos clusters. O número de clusters ideal é definido como o ponto onde o gráfico se aproxima a uma horizontal (como o ponto de encontro do braço e antebraço)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos apresentar o gráfico de inércia do problema anterior e verificar se escolhemos corretamente o número de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(1,8,1)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in k:\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar que o número de clusters ideal é 3. Essa análise nos mostra que escolhemos corretamente o número de clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Cluster Usuários Elo7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já conhecemos o K-Means, vamos voltar para o problema dos usuários do Elo7. Será que conseguimos encontrar clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Veja novamente os dados\n",
    "df_user_elo7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos plotar o gráfico\n",
    "df_user_elo7.plot.scatter(x='tempo',y='ticket', alpha=0.5)\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Ticket (R$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = range(2,5)\n",
    "\n",
    "for n in n_clusters:\n",
    "    estimator = KMeans(n_clusters=n, random_state=170)\n",
    "    estimator.fit(df_user_elo7.values)\n",
    "    labels = estimator.labels_\n",
    "    plt.scatter(x=df_user_elo7.values[:,0],\n",
    "                y=df_user_elo7.values[:,1],\n",
    "                c=labels.astype(np.float),\n",
    "                cmap='rainbow',\n",
    "                edgecolor='k')\n",
    "    plt.title('Num clusters: {}'.format(n))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_user_elo7.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos normalizar os dados!\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# E agora plotamos o resultado\n",
    "n_clusters = range(2,5)\n",
    "\n",
    "X = df_user_elo7.values\n",
    "\n",
    "# Dados normalizados\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "for n in n_clusters:\n",
    "    estimator = KMeans(n_clusters=n, random_state=170)\n",
    "    estimator.fit(X_scaled)\n",
    "    labels = estimator.labels_\n",
    "    plt.scatter(x=X_scaled[:,0],\n",
    "                y=X_scaled[:,1],\n",
    "                c=labels.astype(np.float),\n",
    "                cmap='rainbow',\n",
    "                edgecolor='k')\n",
    "    plt.title('Num clusters: {}'.format(n))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(1,8,1)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in k:\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X_scaled)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o valor da inércia decresce rapidamente até *k*=3, mas depois decresce lentamente. Uma regra usual para a escolha do número de clusters é pegar exatamente esse valor de mudança de inclinação mais acentuada. De fato, ao escolhermos 3 clusters para o problema, nós poderíamos encontrar clusters bem separados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case Elo7 - Clustering de Frete\n",
    "\n",
    "Um dos problemas mais complicados do Elo7 é sua dependência dos correios. Nós sofremos muito com a falta de alternativas para dar aos nossos clientes (compradores e vendedores), já que o serviço dos correios além de caro, é também instável. \n",
    "\n",
    "Para tentar resolver esse problema, o time de Data Science do Elo7 foi chamado para tentar encontrar alguma alternativa. Após algumas conversas, nós levantamos a possibilidade de utilizarmos serviços de entrega independentes dos correios. Mas, o problema é que esses serviços necessitam de um volume grande de encomendas por ponto de coleta, o que não é o caso para a maioria dos vendedores cadastrados no Elo7. \n",
    "\n",
    "Uma possível solução seria encontrar pontos de coleta que pudessem agregar pedidos de vários vendedores e enviar de uma vez só com um desses serviços alternativos. Mas, como obtemos a localização desses pontos de coleta? Podemos aplicar um algoritmo de clustering nas rotas de frete mais frequentes!\n",
    "\n",
    "Vamos tentar analisar os dados e verificar o que conseguimos obter. O dataset a seguir contém pares de endereços de origem e destino de entregas realizadas apenas na cidade de São Paulo em um curto intervalo de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_route = pd.read_csv(os.path.join(DATASET_FOLDER, 'route_clustering_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_route.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar os cálculos de distância, as latitudes e longitudes dos locais já foram realizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora formar nosso vetor de features contendo as posições geográficas das nossas rotas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_route[['latitude_origem','longitude_origem','latitude_destino','longitude_destino']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos normalizar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_route[['latitude_origem','longitude_origem','latitude_destino','longitude_destino']].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos clusters vamos utilizar? Podemos aplicar o método do cotovelo para descobrir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(1,10,1)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in k:\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X_scaled)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos iniciar o algoritmo de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = kmeans.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise da quantidade de ítens em cada cluster é sempre uma boa prática. Clusters desbalanceados é um sinal de que os dados não foram bem separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label, count = np.unique(labels, return_counts=True)\n",
    "for l, c in zip(label,count):\n",
    "    print('Cluster {}: {}'.format(l,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver os gráficos para analisar qualitativamente os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_title('Origem')\n",
    "plt.scatter(x=X[:,0],\n",
    "            y=X[:,1],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax1.set_xlim((-23.4,-23.9))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.set_title('Destino')\n",
    "plt.scatter(x=X[:,2],\n",
    "            y=X[:,3],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax2.set_xlim((-23.4,-23.9))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou? É possível perceber clusters bem definidos? Será que podemos utilizar esses clusters para resolver nossos problemas de frete?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DBSCAN\n",
    "\n",
    "\n",
    "#### Perfil usuários Elo7\n",
    "Vamos ver como ficaria o exemplo dos usuários do Elo7 utilizando DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = df_user_elo7.values\n",
    "\n",
    "# Dados normalizados\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "estimator = DBSCAN(eps=0.33, min_samples=10, metric='euclidean')\n",
    "\n",
    "estimator.fit(X_scaled)\n",
    "\n",
    "labels = estimator.labels_\n",
    "plt.scatter(x=X[:,0],\n",
    "            y=X[:,1],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "plt.show()\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Rotas Frete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.53, min_samples=30, metric='euclidean')\n",
    "\n",
    "X = df_route[['latitude_origem','longitude_origem','latitude_destino','longitude_destino']].values\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_title('Origem')\n",
    "plt.scatter(x=X[:,0],\n",
    "            y=X[:,1],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax1.set_xlim((-23.4,-23.9))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.set_title('Destino')\n",
    "plt.scatter(x=X[:,2],\n",
    "            y=X[:,3],\n",
    "            c=labels, \n",
    "            edgecolor='k',\n",
    "            cmap='rainbow')\n",
    "ax2.set_xlim((-23.4,-23.9))\n",
    "\n",
    "ax1.legend(labels)\n",
    "plt.show()\n",
    "\n",
    "print(np.unique(dbscan.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Hierarchical Clustering\n",
    "\n",
    "Vamos agora aprender sobre outro método de clustering: [**Hierarchical Clustering**](https://en.wikipedia.org/wiki/Hierarchical_clustering). Como o nome mesmo diz, ele utiliza o conceito de *hierarquia* para construir os clusters. Existem duas principais variações do algoritmo: aglomerativo e por divisão. O primeiro é mais usado na prática. O passo a passo do algoritmo é apresentado abaixo:\n",
    "\n",
    "- Primeiro colocamos todos as observações em clusters próprios;\n",
    "- Depois, iterativamente procuramos os clusters mais próximos\\* e agrupamos eles em um novo cluster;\n",
    "- Repetimos o passo anterior até formarmos um único cluster com todas as observações.\n",
    "\n",
    "\\*Obs: A definição de distância (ou similaridade) entre clusters depende do tipo de métrica de distância (Euclidiana, Manhattan, cosseno etc) e ligação (Ward, simples, completa etc).\n",
    "\n",
    "Como podemos ver no algoritmo, o objetivo é a criação de um grande cluster que agrupe todos os dados. Nós podemos visualizar esse histórico de agrupamentos a partir de um [dendrograma](https://en.wikipedia.org/wiki/Dendrogram). A então criação de clusters mais granulares depende da região de similaridade que se deseja realizar o corte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercício prático Hierarchical Clustering\n",
    "\n",
    "Vamos praticar agora!\n",
    "Utilizaremos o dataset do [Eurovision de 2016](https://eurovision.tv/history/full-split-results) para essa tarefa. Esse evento é uma competição de músicas entre países. Cada país participante seleciona uma música para concorrer com as outras. Ao final, cada país deve votar nas músicas com pontuações entre [1,2,3,4,5,6,7,8,10,12]. Um país pode votar tanto através de um juri formado oficialmente pelo país ou via votos por telefone. Ao final, ganha o país que receber a maior quantidade de pontos.\n",
    "\n",
    "Essa competição é famosa por apresentar um comportamento indesejado: os países próximos geografica e culturalmente tendem a se favorecer. Por essa razão há sempre mudanças nas regras para tentar evitar que isso aconteça. Será que conseguiremos perceber esse comportamento através de um algoritmo de clustering? Vamos tentar!\n",
    "\n",
    "Um país não pode votar em si próprio, mas, para podermos fazer a nossa análise, considerei que um país daria a pontuação máxima (12) para ele próprio.\n",
    "\n",
    "As colunas do dataset são descritas a seguir e consideramos apenas votos feitos por telefone:\n",
    "\n",
    "- `From country`: País votante\n",
    "- `Televote Points`: Pontuação dada por telefone pelo país votante\n",
    "- `To country`: País que recebeu a pontuação do país votante\n",
    "\n",
    "![Eurovision](https://www.eurovisionary.com/wp-content/uploads/2015/11/eurovision-2016.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos iniciar a leitura do dataset\n",
    "df_euro = pd.read_csv(os.path.join(DATASET_FOLDER, 'eurovision_dataset.csv'), sep=';')\n",
    "\n",
    "df_euro.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Podemos visualizar quem está ganhando nessa votação\n",
    "df_euro_points = df_euro.groupby(['To country']).sum()\n",
    "df_euro_points.sort_values(by='Televote Points', inplace=True, ascending=False)\n",
    "df_euro_points.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se formos considerar apenas os votos por telefone, a Russia está ganhando a competição!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fizemos para o exercício anterior, nós temos que tomar cuidado com a distribuição dos dados. como temos valores discretos e determinísticos de pontuação, não faz sentido analisarmos a variância dos dados. Entretanto, podemos normalizar os pontos em uma escala de 0-100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtenha o vetor de pontos\n",
    "X = np.array([i for i in df_euro.groupby('From country')['Televote Points'].apply(np.array)])\n",
    "\n",
    "# TODO\n",
    "# Realize a normalização dos dados\n",
    "# Dica: Procure por MaxAbsScaler no scikit-learn\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "normalizer = MaxAbsScaler()\n",
    "\n",
    "X_norm = normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos realizar agora o algoritmo Hierarchical Clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Aplique o algoritmo Hierarchical Clustering utilizando o scipy\n",
    "# Selecione uma métrica de distância e um método de ligação\n",
    "# Teste vários para obter a intuição por trás de cada método\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "Y = linkage(X_norm, method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Crie o dendrograma para visualizar os resultados\n",
    "# Será necessário obter os valores de labels que são os países\n",
    "# *votantes* da competição (Dica: Procure pelo método unique do pandas)\n",
    "plt.figure(figsize=(16,10))\n",
    "labels = pd.unique(df_euro['From country'])\n",
    "dendrogram(Y, \n",
    "           labels=labels, \n",
    "           leaf_rotation=90, \n",
    "           leaf_font_size=14) # Use o leaf_font_size = 14\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que podemos dizer sobre o dendrograma? Há alguma relação cultural, política ou geográfica entre os países? Se quiséssemos escolher uma região de corte para formar clusters intermediários, qual seria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo Grãos (Opcional)\n",
    "\n",
    "Vamos utilizar o dataset de diferentes tipos de grãos de trigo obtidos pelo [UCI](https://archive.ics.uci.edu/ml/datasets/seeds#) para treinar esses conceitos.\n",
    "\n",
    "O dataset contém os seguintes parâmetros:\n",
    "\n",
    "- `area`: Área total do grão, A\n",
    "- `perimeter`: Perímetro do grão, P\n",
    "- `compactness`: Grão de compactação do grão - $C = \\frac{4 \\pi A}{P^2}$\n",
    "- `length_kernel`: Comprimento do núcleo\n",
    "- `width_kernel`: Largura do núcleo\n",
    "- `asymmetry`: Coeficiente de assimetria\n",
    "- `kernel_groove`: Comprimento do sulco do núcleo\n",
    "\n",
    "Variedades de grãos: 'Kama' (1), 'Rosa' (2) e 'Canadian' (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importar os dados\n",
    "df_grain = pd.read_csv(os.path.join(DATASET_FOLDER, 'seeds_dataset.csv'), sep=';')\n",
    "df_grain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de iniciarmos o processo de clustering, vamos verificar se há muita discrepância entre as variâncias das features. Essa etapa é muito importante, porque features com variância elevada possuem maior influência na medida de distância do algoritmo do que features com menor variância, o que pode ser indesejado. Quando normalizamos as features, nós conseguimos dar influências iguais para todas elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grain.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que o atributo `area` possui maior variância, enquanto o `compactness` possui baixa variância. Portanto, vamos primeiramente normalizar as features utilizando o [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) do scikit-learn. Ele normaliza as features individualmente deixando a variância delas igual a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos importar o StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos criar agora o vetor de atributos\n",
    "X = df_grain[['area','perimeter','compactness','length_kernel','width_kernel','asymmetry','kernel_groove']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agora precisamos normalizar o vetor de features\n",
    "\n",
    "# Primeiro criamos uma instância do StandardScaler\n",
    "normalizer = StandardScaler()\n",
    "\n",
    "# Agora podemos normalizar através do método fit_transform\n",
    "X_norm = normalizer.fit_transform(X)\n",
    "\n",
    "# Podemos verificar que a variância de cada feature é 1\n",
    "np.var(X_norm, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O scikit-learn possui um método próprio para o algoritmo de [Hierarchical Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering). Entretanto, ele não nos permite visualizar facilmente o dendrograma final. Por isso, vamos utilizar a versão do scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importe os métodos linkage (Hierarchical Clustering) e dendrogram\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos escolher a métrica de distância:\n",
    "distance = 'euclidean'\n",
    "# Agora o tipo de ligação\n",
    "linkage_type = 'complete'\n",
    "\n",
    "# Vamos aplicar o método linkage\n",
    "Y = linkage(X_norm, method=linkage_type, metric=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agora estamos prontos para plotar o dendrograma\n",
    "# Vamos obter o nome das variedades dos grãos\n",
    "varieties = df_grain['varieties'].values\n",
    "\n",
    "# Construímos finalmente o dendrograma\n",
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que os clusters formados a partir do Hierarchical Clustering fazem bastante sentido com relação às variedades dos grãos de trigo. Vamos realizar agora um corte no dendrograma de forma a ficarmos com apenas 3 clusters, que representa uma distância de 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para essa tarefa nós podemos usar o método [`fcluster`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.fcluster.html) do scipy. Ele nos permite realizar um corte na árvore de clustering gerada pelo Hierarchical Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos gerar os rótulos para os clustes\n",
    "num_clusters = 3\n",
    "labels = fcluster(Y, num_clusters ,criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos agora criar um dataframe para podermos utilizar o cross-tabulation do pandas\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Crie a matriz de tabulação cruzada\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que o as variedades Canadian e Rosa foram muito bem agrupados nos clusters, mas a Kama não teve o mesmo sucesso. Vamos tentar utilizar o método \"Ward\" para a ligação e ver se há alguma variação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distância\n",
    "distance = 'euclidean'\n",
    "# Ligação\n",
    "linkage_type = 'ward'\n",
    "\n",
    "# Treinar modelo Hierarchical Clustering\n",
    "Y = linkage(X_norm, method=linkage_type, metric=distance)\n",
    "\n",
    "# Plota dendrograma\n",
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Número de clusters\n",
    "num_clusters = 3\n",
    "\n",
    "# Obtem labels\n",
    "labels = fcluster(Y, num_clusters ,criterion='maxclust')\n",
    "\n",
    "# Vamos agora criar um dataframe para podermos utilizar o cross-tabulation do pandas\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Crie a matriz de tabulação cruzada\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos um resultado melhor para a variedade Kama, mas piorou um pouco o resultado para a Canadian. Dificilmente teremos um algoritmo que consegue ser perfeito em todos os casos. Mas, como podemos visualizar no dendrograma e na matriz de tabulação cruzada, nós conseguimos um bom resultado de clusterização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Comparação métodos clustering:\n",
    "### K-Means x Hierarchical Clustering x DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar os 3 algoritmos utilizando alguns datasets padrão do scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Gera os dados\n",
    "# ============\n",
    "n_samples = 1500\n",
    "# Circulos concentricos\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "# Formato de lua\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "# Bolas\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "# Sem estrutura (uniforme)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Dados distribuídos anisotropicamente\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Ajusta os parametros dos modelos e datasets\n",
    "# ============\n",
    "plt.figure(figsize=(16, 20))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "# Parametros dos modelos de clustering\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "# Parametros do dataset\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # Atualiza os parametros para o dataset especifico\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # Normaliza o dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # ============\n",
    "    # Criação dos objetos de clustering\n",
    "    # ============\n",
    "    \n",
    "    # K-Means\n",
    "    two_means = cluster.KMeans(n_clusters=params['n_clusters'])\n",
    "    \n",
    "    # DBSCAN\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    \n",
    "    # Hierarchical Clustering - Aglomerativo\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        linkage=\"ward\",\n",
    "        n_clusters=params['n_clusters'])\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('KMeans', two_means),\n",
    "        ('HierarchicalClustering', ward),\n",
    "        ('DBSCAN', dbscan),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        algorithm.fit(X)\n",
    "\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Elo7 - Motivos de Compra\n",
    "\n",
    "Os compradores do Elo7 são incentivados a indicar o motivo da compra de determinado produto no seu marketplace. Esses motivos nos ajudam a entender melhor o **momento** de compra do usuário. O dataset apresentado a seguir contém um subset desses motivos de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reason = pd.read_csv(os.path.join(DATASET_FOLDER, 'purchase_reason_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_reason.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem muitos tipos possíveis de motivos de compra, mas será que nós podemos encontrar algum padrão neles? Me parece um problema clássico de **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o Tf-Idf para criar o embedding dos motivos de compra e o K-Means para encontrar clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.9, max_features=5000, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria a matriz de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_reason['reason'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como escolher o número de clusters? Vamos utilizar o gráfico de inércias. (Obs: outra possibilidade é avaliar o [\"silhouette score\"](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(10,200,20)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in tqdm(k):\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializa o K-Means com a quantidade de clusters que escolhemos a partir do gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treina o modelo K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontra os clusters para cada motivo de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora visualizar os clusters criados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crie um novo dataframe com os labels dos clusters\n",
    "df = pd.DataFrame({'reason': df_reason['reason'], 'labels': labels})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar a distribuição de motivos em cada cluster. Quanto mais desbalanceado, pior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('labels').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar alguns exemplos de clusters gerados pelo K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(50):\n",
    "    idx_labels = df[df['labels']==idx]['reason'].unique()\n",
    "    print('- Cluster {}:'.format(idx + 1))\n",
    "    for i in np.random.choice(idx_labels, min(len(idx_labels), 10), replace=False):\n",
    "        print(' '*5, i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar agora se existe alguma hierarquia entre os clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "Y = linkage(kmeans.cluster_centers_, method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = df.groupby('labels').apply(lambda x: np.random.choice(list(x['reason'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=titles.values,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que há uma hierarquia entre os clusters. Poderíamos usar os métodos apresentados anteriormente para agrupar os clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Elo7 - Subcategorias Automáticas\n",
    "\n",
    "Vamos para mais um case real do Elo7!\n",
    "\n",
    "Esse case é um dos trabalhos mais recentes do time de Data Science do Elo7. De fato, é um trabalho ainda em aberto e qualquer sugestão de melhorias é bem vinda! =)\n",
    "\n",
    "- O problema:\n",
    "O Elo7 possui uma árvore de categorias dividida em N1 e N2. O primeiro nível (N1) contém as categorias \"alto nível\" do site. São as categorias mais genéricas do marketplace- ou, pelo menos, é assim gostaríamos que fosse. As categorias N2, ou subcategorias, são as possíveis extensões dos nós das categorias N1. Podemos perceber que a árvore é extremamente limitada e isso é um problema grave não só para os compradores, que não conseguem navegar nas nossas categorias, mas também para os vendedores, que não conseguem categorizar bem seus produtos. A solução para esse problema seria uma árvore de categorias com maior \"granularidade\", ou seja, que consiga expandir além dos 2 níveis e ter mais subcategorias.\n",
    "\n",
    "- O que o time de Data Science tem a ver com essa história? \n",
    "\n",
    "Bom, gerar uma nova árvore de categoria pode ser uma tarefa bastante monótona e cansativa. Provavelmente deve haver algum jeito de encontrar bons agrupamentos de produtos que pudessem servir como uma nova subcategoria. Talvez algum método de clustering que utilize como features o conteúdo dos produtos pode gerar algum resultado interessante.\n",
    "\n",
    "- O experimento:\n",
    "\n",
    "O dataset a seguir possui um subconjunto de produtos que foram categorizados na categoria N1 \"Casamento\". Escolhemos esse conjunto de dados para iniciar nossos trabalhos, porque assim temos mais controle sobre nossos resultados. E, também, porque é uma das categorias mais importantes do marketplace.\n",
    "\n",
    "Para essa tarefa, vamos utilizar apenas o título e uma parte da descrição do produto (aprox. 140 caracteres) como features de entrada.\n",
    "\n",
    "Vamos analisar os dados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cat = pd.read_csv(os.path.join(DATASET_FOLDER, 'subcategory_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma coluna com as features que vamos incluir no nosso modelo de aprendizagem.\n",
    "Esse vetor de features será o título + descrição do produto. Para compensar a quantidade de palavras do título em relação a descrição, vamos repetir o título duas vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cat['title_desc'] = (df_cat['title'] + ' ')*2 + df_cat['short_description']\n",
    "\n",
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar nossa matriz de features, nós vamos utilizar o Tf-Idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.9, max_features=10000, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_cat['title_desc'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, precisamos definir o número de clusters. Podemos utilizar o método do gráfico de inércias.\n",
    "\n",
    "(Obs: O cálculo pode levar muito tempo para ser executado. Assuma que o valor escolhido é 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Range de valores de clusters que vamos testar\n",
    "k = range(10,100,10)\n",
    "\n",
    "# Lista de inércias\n",
    "inertias = []\n",
    "\n",
    "# Para cada valor de k, ache a inércia\n",
    "for i in tqdm(k):\n",
    "    # crie a instância\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "\n",
    "    # Treine o modelo\n",
    "    model = kmeans.fit(X)\n",
    "\n",
    "    # Ache a inercia dos clusters\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(k, inertias, '-ob')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Método utilizando cálculo paralelo - Descomente as linhas para usar\n",
    "\n",
    "# # Range de valores de clusters que vamos testar\n",
    "# k_range = range(10,100,10)\n",
    "\n",
    "# # Baixe o joblib, se não tiver no pc\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# # Para cada valor de k, ache a inércia\n",
    "# def find_kmeans_inertia(k, X):\n",
    "#     # crie a instância\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "#     # Treine o modelo\n",
    "#     model = kmeans.fit(X)\n",
    "\n",
    "#     # Ache a inercia dos clusters\n",
    "#     return model.inertia_\n",
    "    \n",
    "# inertias = Parallel(n_jobs=-1, verbose=200)(delayed(find_kmeans_inertia)(k, X) for k in k_range)\n",
    "\n",
    "# plt.plot(k, inertias, '-ob')\n",
    "# plt.xlabel('Clusters')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title': df_cat['title'], 'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('labels').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(len(df['labels'].unique())):\n",
    "    idx_labels = df[df['labels']==idx]['title'].unique()\n",
    "    print('- Cluster {}:'.format(idx + 1))\n",
    "    for i in np.random.choice(idx_labels, min(len(idx_labels), 10), replace=False):\n",
    "        print(' '*5, i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "Y = linkage(kmeans.cluster_centers_, method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = df.groupby('labels').apply(lambda x: np.random.choice(list(x['title'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "dendrogram(Y,\n",
    "           labels=titles.values,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
