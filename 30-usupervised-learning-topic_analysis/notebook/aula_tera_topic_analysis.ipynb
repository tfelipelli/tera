{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERA - Aula 30\n",
    "## Redução de Dimensionalidade e Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivos gerais de topic analysis:\n",
    "- Agrupar dados de forma a criar representações sumarizadas (sumarização de dados)\n",
    "- Redução de dimensionalidade\n",
    "- Clusters encontrados são explicados (encontra tópicos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "\n",
    "- [Exemplo inicial](#Exemplo-Inicial)\n",
    "- [PCA](#Principal-Component-Analysis-(PCA)\n",
    "- [T-SNE](#T-SNE)\n",
    "- [Topic Analysis](#Topic-Analysis)\n",
    "    - [NMF](#Non-Negative-Matrix-Factorization-(NMF)\n",
    "    - [LDA](#Latent-Dirichlet-Allocation-(LDA)\n",
    "- [Case Subcategorias Elo7](#Case-Elo7---Subcategorias-Automáticas)\n",
    "- [Case Sistemas de Recomendação](#Case-Sistema-de-Recomendação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports usados no curso\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "plt.rcParams['figure.figsize'] = (16.0, 10.0)\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pasta contendo os dados:\n",
    "ROOT_FOLDER = os.path.realpath('..')\n",
    "DATASET_FOLDER = os.path.join(ROOT_FOLDER,'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exemplo inicial\n",
    "- **Case Elo7: Clustering de Frete**\n",
    "\n",
    "Vamos voltar para o problema de agrupamento de rotas para melhoria do frete no Elo7. Nosso vetor de atributos tem dimensão 4, o que nos impossibilita de observá-lo em apenas um gráfico. Essa limitação é um problema tanto do ponto de vista de intuição sobre o problema, quanto da correta observação dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_route = pd.read_csv(os.path.join(DATASET_FOLDER, 'route_clustering_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_route.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar os cálculos de distância, as latitudes e longitudes dos locais já foram realizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos observar a distribuição dos nossos dados. Como temos 4 dimensões, uma forma de fazer isso seria uma matriz de gráficos pareados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_route[['latitude_origem','longitude_origem','latitude_destino','longitude_destino']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, 4 dimensões ainda é possível ter uma boa intuição sobre o problema. Mas, e mais do que 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo 2\n",
    "- **Variedades de grãos de trigo**\n",
    "\n",
    "Vamos utilizar o dataset de diferentes tipos de grãos de trigo obtidos pelo [UCI](https://archive.ics.uci.edu/ml/datasets/seeds#).\n",
    "\n",
    "<img src=\"https://kaggle2.blob.core.windows.net/datasets-images/904/1650/133a751e78dabf09031d6adfa0075ae0/dataset-original.jpg\" width=\"400\" height=\"100\" />\n",
    "\n",
    "Esse dataset contém medidas de raio-x de três variedades de grãos de trigo. Gostaríamos de \n",
    "\n",
    "O dataset contém os seguintes parâmetros:\n",
    "\n",
    "- `area`: Área total do grão, A\n",
    "- `perimeter`: Perímetro do grão, P\n",
    "- `compactness`: Grão de compactação do grão - $C = \\frac{4 \\pi A}{P^2}$\n",
    "- `length_kernel`: Comprimento do núcleo\n",
    "- `width_kernel`: Largura do núcleo\n",
    "- `asymmetry`: Coeficiente de assimetria\n",
    "- `kernel_groove`: Comprimento do sulco do núcleo\n",
    "\n",
    "Variedades de grãos: 'Kama' (1), 'Rosa' (2) e 'Canadian' (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos um dataset com 7 dimensões. As coisas estão ficando mais complicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importar os dados\n",
    "df_grain = pd.read_csv(os.path.join(DATASET_FOLDER, 'seeds_dataset.csv'), sep=';')\n",
    "df_grain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos aplicar agora a matriz de gráficos pareados para tentar visualizar o problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_grain, hue='varieties')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos agora sofrendo um pouco mais para visualizar os dados. O número de gráficos cresce ao quadrado em relação ao número de dimensões!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exemplo 3\n",
    "- **Case Elo7 - Motivos de Compra**\n",
    "\n",
    "Percebemos até agora que quanto maior o número de dimensões, menor nossa capacidade de observar e gerar intuição sobre o problema. O que podemos fazer agora com um problema de NLP, onde normalmente temos milhares de dimensões (cada palavra do vocabulário representa uma dimensão)?\n",
    "\n",
    "Vamos verificar no dataset de motivos de compra dos usuários do Elo7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reason = pd.read_csv(os.path.join(DATASET_FOLDER, 'purchase_reason_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_reason.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o Tf-Idf para criar o embedding dos motivos de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.9, max_features=500, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria a matriz de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_reason['reason'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que a matriz de atributos agora tem 496 dimensões! Para representar essas dimensões em uma matriz de gráficos pareados, nós precisaríamos de $496^2=246016$ gráficos! Totalmente impraticável!\n",
    "\n",
    "Mas, a questão que deveria ser levantada é: Será que todas essas dimensões são necessárias para representar **fielmente** o problema? Será que não conseguiríamos simplificar de alguma forma? Isso é, não seria possível remover ou modificar certas dimensões que sejam \"menos importantes\" sem remover muita informação?\n",
    "- Resposta: Sim, é possível!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "O [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) é uma das técnicas mais utilizadas em Machine Learning para encontrar representações sumarizadas sobre os problemas de alta dimensionalidade. O PCA procura descorrelacionar linearmente os atributos do sistema, encontrando assim os **componentes principais**. Cada componente pode ser visto como um novo vetor de atributos que é formado pela combinação linear dos atributos originais. Os componentes principais são ordenados de forma que o primeiro componente representa a região de maior variância do sistema, o segundo representa a segunda maior e é ortogonal ao primeiro, e assim sucessivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos aplicar o método PCA nos problemas anteriores para verificar se conseguimos visualizar melhor o problema. Também podemos tentar reduzir o número de dimensões do nosso problema.\n",
    "\n",
    "O algoritmo básico do PCA realiza 4 passos fundamentais para transformar o espaço de atributos e achar os componentes princiais:\n",
    "\n",
    "1. Remove a média amostral\n",
    "2. Rotaciona os eixos para descorrelacionar os dados\n",
    "3. Ordena os componentes principais por nível de variância (importância)\n",
    "4. Elimina os componentes menos variantes (Opcional - redução de dimensionalidade)\n",
    "\n",
    "A implementação do PCA no sklearn pode ser encontrada neste [link](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elo7: Clustering Frete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_route = pd.read_csv(os.path.join(DATASET_FOLDER, 'route_clustering_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_route.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df_route[['latitude_origem','longitude_origem','latitude_destino','longitude_destino']].values\n",
    "\n",
    "# Normaliza os dados\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos agora importar o método PCA do scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Criemos agora a instância PCA\n",
    "# Não definimos o número de features: não realiza passo 4\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que ainda temos 4 dimensões. O PCA não reduziu a dimensionalidade do problema. Esse passo pode ser realizado posteriormente. O que temos agora é uma relação de \"importância\" entre os componentes principais. Vamos plotar o gráfico com apenas os dois primeiros componentes para verificar o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.8, edgecolor='k')\n",
    "plt.xlabel('Primeiro componente principal')\n",
    "plt.ylabel('Segundo componente principal')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos observar como ficariam os clusters encontrados na aula de Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encontra os clusters da aula anterior\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "labels = kmeans.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='rainbow', alpha=0.8, edgecolor='k')\n",
    "plt.xlabel('Primeiro componente principal')\n",
    "plt.ylabel('Segundo componente principal')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou? Esses dois eixos nos mostram bons detalhes sobre os clusters encontrados anteriormente.\n",
    "\n",
    "Outra coisa que podemos fazer é verificar o quanto cada eixo representa da variância do modelo, ou também chamado de **variância explicada** pelo componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Componentes principais\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "plt.bar(features, pca.explained_variance_ratio_)\n",
    "plt.xticks(features)\n",
    "plt.xlabel('Componentes principais')\n",
    "plt.ylabel('Variância')\n",
    "plt.show()\n",
    "\n",
    "print('Variância explicada: ')\n",
    "cum_exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "print('-'*50)\n",
    "for i in range(len(pca.explained_variance_ratio_)):\n",
    "    print('{} Componente(s) principal(is): {:.2%}'.format(i+1, cum_exp_var[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber que os dois primeiros eixos em conjunto representam cerca de 63% da variância explicada. Isso ainda é pouco, mas já foi possível verificar uma boa representação dos dados. Outro fato a ser observado é que o quarto eixo não possui grande influência no sistema. Se retirássemos ele, ainda teríamos 3 eixos que explicariam 82% da variância do sistema. Se não estivermos preocupados com uma grande precisão, poderíamos retirar esse componente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variedade de grãos\n",
    "\n",
    "Vamos praticar para um problema com mais dimensões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importar os dados\n",
    "df_grain = pd.read_csv(os.path.join(DATASET_FOLDER, 'seeds_dataset.csv'), sep=';')\n",
    "df_grain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "X_pca = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dica: Transforme as variedades categorias em valores numéricos -> LabelEncoder (scklearn)\n",
    "labels = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='rainbow', alpha=0.8, edgecolor='k')\n",
    "plt.xlabel('Primeiro componente principal')\n",
    "plt.ylabel('Segundo componente principal')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Verifique a variância explicada por cada componente.\n",
    "# Podemos reduzir a dimensionalidade do nosso problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elo7: Motivo de compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reason = pd.read_csv(os.path.join(DATASET_FOLDER, 'purchase_reason_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_reason.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.9, max_features=500, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_reason['reason'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O TF-IDF cria uma representação esparsa para a matriz de atributos. Isso se deve ao fato de que existem muito mais \"zeros\" do que números na nossa matriz. Para isso, precisamos usar a função [`TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) do sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=X.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_svd = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrar a variância explicada pelos componentes principais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Componentes principais\n",
    "features = range(svd.components_.shape[1]-1)\n",
    "\n",
    "# Ordena os componentes - diferente do PCA que já vem ordenado\n",
    "exp_var = np.sort(svd.explained_variance_ratio_)[::-1]\n",
    "\n",
    "# Apresenta apenas os 50 primeiros\n",
    "plt.bar(features[:50], exp_var[:50])\n",
    "plt.xticks(features[:50])\n",
    "plt.xlabel('Componentes principais')\n",
    "plt.ylabel('Variância')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Variância explicada: ')\n",
    "cum_exp_var = np.cumsum(exp_var)[:100]\n",
    "\n",
    "print('-'*50)\n",
    "for i in range(0,len(exp_var[:100]),10):\n",
    "    print('{} Componente(s) principal(is): {:.2%}'.format(i+1, cum_exp_var[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar que conseguiríamos reduzir consideravelmente a dimensão do problema! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar o gráfico dos primeiros componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_svd[:, 0], X_svd[:, 1], alpha=0.8, edgecolor='k')\n",
    "plt.xlabel('Primeiro componente principal')\n",
    "plt.ylabel('Segundo componente principal')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos plotar também os centroides dos clusters encontrados utilizando o K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utiliza o K-Means para encontrar os clusters\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "# Cria um dataframe com as labels\n",
    "df = pd.DataFrame({'reason': df_reason['reason'], 'labels': labels})\n",
    "\n",
    "# Escolhe um título para o cluster\n",
    "## para cada cluster, escolhe aleatoriamente um motivo de compra\n",
    "titles = df.groupby('labels').apply(lambda x: np.random.choice(list(x['reason'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cria uma matriz representando os centroides dos clusters\n",
    "X_centers_svd = svd.transform(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plota os centroides dos clusters\n",
    "plt.scatter(X_centers_svd[:, 0], \n",
    "            X_centers_svd[:, 1], \n",
    "            c=range(X_centers_svd.shape[0]), \n",
    "            cmap='rainbow', \n",
    "            alpha=0.8, \n",
    "            edgecolor='k')\n",
    "plt.xlabel('Primeiro componente principal')\n",
    "plt.ylabel('Segundo componente principal')\n",
    "plt.grid()\n",
    "\n",
    "# Coloca nomes nos pontos\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    plt.annotate(titles[i], \n",
    "                 ((X_centers_svd[i, 0], X_centers_svd[i, 1])),\n",
    "                 va=\"bottom\", ha=\"left\", rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora estamos próximos do que gostaríamos! Em um gráfico já conseguimos ver mais informações sobre nossos dados de alta dimensionalidade!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# T-SNE\n",
    "\n",
    "[T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) é um algoritmo de machine learning iterativo que procura mapear regiões multidimensionais em 2 ou 3 dimensões. O resultado gerado pelo método é um mapa de dispersão na qual os pontos com maior afinidade (mesma estrutura) estão próximos, enquanto os pontos dissimilares ficam distantes. Note que os eixos não têm necessariamente um significado predeterminado. Não podemos calcular distâncias entre pontos a partir do resultado do T-SNE.\n",
    "\n",
    "Normalmente, o T-SNE é utilizado para visualização de dados. Podemos mapear problemas de alta dimensionalidade em 2 ou 3 dimensões, onde garantimos que a sua estrutura se manterá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos agora importar o método T-SNE do scikit-learn\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset variedade de grãos de trigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_grain.drop('varieties',1).values\n",
    "\n",
    "X_scaled = normalizer.fit_transform(X)\n",
    "\n",
    "# Codifica variedades em valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "varieties = label_encoder.fit_transform(df_grain['varieties'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute o T-SNE\n",
    "# Crie a instância do T-SNE\n",
    "# Teste diferentes valores de learning_rate e perplexity\n",
    "tsne = TSNE(learning_rate=200, perplexity=30)\n",
    "\n",
    "# Transforme os dados\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Agora basta plotar o resultado\n",
    "plt.scatter(X_tsne[:,0], \n",
    "            X_tsne[:,1], \n",
    "            c=varieties, \n",
    "            cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artigos do Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset que vamos utilizar foi retirado desse [link](https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/). O dataset original contém 463 mil páginas do Wikipedia de diversos temas. Para não ficarmos o dia inteiro processando os dados, foram retirados apenas 58 artigos de classes bem definidas (por exemplo, futebol, música, internet etc). Vamos analisar o dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(os.path.join(DATASET_FOLDER, 'wikipedia_dataset.csv'), sep=';')\n",
    "\n",
    "df_wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A coluna de clusters foi feita de maneira a facilitar nosso entendimento sobre o resultado final do algoritmo. Na maioria dos casos nós não teremos esse entendimento antes de realizar uma primeira análise (clustering) dos algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar aplicar os métodos PCA para reduzir a dimensionalidade desse problema e o T-SNE para visualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas, primeiro, precisamos criar a matriz de atributos. Para isso, vamos utilizar o TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.8, max_features=15000, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aplique a transformação nos artigos\n",
    "X = tfidf.fit_transform(df_wiki['artigo'].values)\n",
    "\n",
    "# Tamanho do dataset\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como temos 15000 dimensões, nós podemos ter problemas com a tal da *maldição da dimensionalidade*. Além disso, a probabilidade de cada uma das palavras estar dentro de todos os documentos é muito baixa. Ou seja, a matriz de atributos é muito esparsa. Isso nos dá um indício forte para acreditar que teremos grandes vantagens se utilizarmos uma técnica para reduzir a dimensionalidade, como o PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos utilizar o TruncatedSVD para reduzir a dimensionalidade do dataset.\n",
    "# Podemos limitar o número de components para ser igual ao número\n",
    "# de dados: 58\n",
    "# Desse modo temos uma matriz quadrada\n",
    "svd = TruncatedSVD(n_components=58)\n",
    "\n",
    "# Aplique o método SVD\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "# Vamos ver como ficou o novo tamanho\n",
    "X_svd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma redução bastante drástica! Reduzimos de 15000 dimensões para apenas 58. Será que perdemos muita informação? Vamos verificar o gráfico de variância explicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Componentes principais\n",
    "features = range(svd.components_.shape[1]-1)\n",
    "\n",
    "# Ordena os componentes - diferente do PCA que já vem ordenado\n",
    "exp_var = np.sort(svd.explained_variance_ratio_)[::-1]\n",
    "\n",
    "# Apresenta apenas os 50 primeiros\n",
    "plt.bar(features[:50], exp_var[:50])\n",
    "plt.xticks(features[:50])\n",
    "plt.xlabel('Componentes principais')\n",
    "plt.ylabel('Variância')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Variância explicada: ')\n",
    "cum_exp_var = np.cumsum(exp_var)\n",
    "\n",
    "print('-'*50)\n",
    "for i in range(0,len(exp_var),10):\n",
    "    print('{} Componente(s) principal(is): {:.2%}'.format(i+1, cum_exp_var[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que ao utilizar 50 componentes, nós já teríamos aproximadamente 90% da variância explicada. Isso nos mostra que poderíamos reduzir ainda mais o número de dimensões escolhidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora plotar o gráfico T-SNE para verificar a estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(learning_rate=5)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = LabelEncoder().fit_transform(df_wiki['cluster'].values)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0],\n",
    "            X_tsne[:, 1],\n",
    "            c=labels,\n",
    "            cmap='rainbow')\n",
    "\n",
    "for i in range(X_tsne.shape[0]):\n",
    "    # Anota os motivos no gráfico\n",
    "    plt.annotate(df_wiki['titulo'][i], \n",
    "                 ((X_tsne[i, 0], X_tsne[i, 1])),\n",
    "                 xytext=(4, 3), \n",
    "                 textcoords='offset points')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós conseguímos apresentar dados de 15000 dimensões em apenas duas e, ainda assim, verificar a presença de uma estrutura bem definida entre os clusters. Incrível, não?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Elo7: Motivos de compra\n",
    "\n",
    "Agora tente aplicar os métodos PCA e T-SNE para encontrar relações entre os dados. Utilize as ferramentas apresentadas anteriormente para gerar um bom entendimento sobre o problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_reason['reason'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplique os métodos PCA e T-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# \n",
    "X_svd = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Encontre a quantidade de dimensões necessárias para o nosso problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Coloque o número de componentes escolhido anteriormente\n",
    "svd = TruncatedSVD(n_components=___)\n",
    "\n",
    "# Aplique o método SVD em um sample dos dados\n",
    "df_sample = df_reason.sample(n=75)\n",
    "X_sample = tfidf.transform(df_sample['reason'].values)\n",
    "\n",
    "X_svd = svd.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Execute o T-SNE\n",
    "# Teste diferentes valores de learning_rate e perplexity\n",
    "tsne = TSNE(___)\n",
    "\n",
    "X_tsne = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como fizemos para o PCA (ou TruncatedSVD), podemos plotar os clusters obtidos pelo K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apenas para auxiliar a verificação dos clusters\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(X_svd)\n",
    "labels = kmeans.predict(X_svd)\n",
    "\n",
    "# Cria um dataframe com as labels\n",
    "df = pd.DataFrame({'reason': df_sample['reason'], 'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apresenta o resultado do T-SNE de duas dimensões com as cores\n",
    "# indicando cada um dos clusters pré-definidos\n",
    "labels_sample = df['labels'].values\n",
    "titles_sample = df['reason'].values\n",
    "\n",
    "plt.scatter(X_tsne[:, 0],\n",
    "            X_tsne[:, 1],\n",
    "            c=labels_sample,\n",
    "            cmap='rainbow')\n",
    "\n",
    "for i in range(X_tsne.shape[0]):\n",
    "    # Anota os motivos no gráfico\n",
    "    plt.annotate(titles_sample[i][:100], \n",
    "                 ((X_tsne[i, 0], X_tsne[i, 1])),\n",
    "                 xytext=(4, 3), \n",
    "                 textcoords='offset points')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analysis\n",
    "\n",
    "Até agora, nós utilizamos diversos algoritmos de clustering e redução de dimensionalidade para conseguir encontrar relações de proximidade entre documentos. Entretanto, apesar de algoritmos como o PCA conseguirem representar reduzidamente o nosso conjunto de documentos, nós não conseguíamos interpretar o resultado obtido. Isso acontece porque o PCA encontra novos vetores de features que são combinações lineares do conjunto de palavras existentes. Esse fator pode não ser um problema se o que se deseja é apenas encontrar clusters sem interpretações mais profundas. Entretanto, muitas vezes gostaríamos de entender o racional por trás da geração dos clusters. Ainda mais, as vezes gostaríamos de reduzir um documento a um conjunto de palavras-chave que podem \"resumir\" o nosso documento e agrupá-las em **tópicos**. E é exatamente esse o objetivo dessa aula.\n",
    "\n",
    "A área de topic analysis é de grande importância para Machine Learning, ou mais especificamente a área de Data Mining. A utilização de tópicos nos permite ter uma melhor e mais compacta representação dos nossos dados, principalmente quando temos um conjunto extenso de dados e atributos (features). \n",
    "\n",
    "Utilizar topic analysis em Natural Language Processing (NLP) é algo bem intuitivo. Nós naturalmente fazemos isso quando queremos organizar textos (documentos) em diferentes categorias, ou temas. Por exemplo, nós podemos ler artigos do Google News e dizer facilmente que um determinado artigo tem o tema \"esporte\", ou o tema \"política\". Nosso trabalho em topic analysis é o de conseguir desenvolver algoritmos de Machine Learning que possam encontrar automaticamente esses tópicos, ou temas, por nós."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "O [NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) é um algoritmo poderoso (apesar de relativamente simples) para encontrar tópicos em um conjunto de documentos e features. Ele se baseia em um processo de decomposição de matrizes para criar uma representação adequada da matriz de frequência de palavras (denotado por **A**). Mais especificamente, o NMF decompõe a matriz de frequência de palavras em duas: a primeira é a matriz de pesos (chamada de **W**), com as linhas representando os documentos e as colunas indicando os tópicos; e a segunda é a matriz de atributos (chamada de **H**), com as linhas indicando os tópicos e as colunas os atributos. O número de tópicos é definido antecipadamente e é fixo.\n",
    "\n",
    "<img src=\"./imagens/nmf_draw.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "As duas matrizes são formadas a partir de um processo iterativo de otimização (veja esse [link](http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf) para mais detalhes) com o objetivo de reconstruir fielmente a matriz **A**. Entretanto, para esse fim, a matriz **A** não pode possuir entradas negativas.\n",
    "\n",
    "Vamos aplicar esse método na prática para verificar o resultado!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exemplo Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(os.path.join(DATASET_FOLDER, 'wikipedia_dataset.csv'), sep=';')\n",
    "\n",
    "df_wiki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.8, max_features=15000, sublinear_tf=True, use_idf=True)\n",
    "\n",
    "X = tfidf.fit_transform(df_wiki['artigo'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Primeiro, importe o módulo NMF do scikit-learn\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Precisamos criar a instância do NMF\n",
    "# Temos que definir um número de componentes para o NMF\n",
    "# Como temos 6 clusters, vamos escolher n_components=6\n",
    "nmf = NMF(n_components=6)\n",
    "\n",
    "# Agora vamos utilizar os mesmos atributos fit, transform ou fit_transform\n",
    "# que já conhecemos do universo do sklearn\n",
    "W_nmf = nmf.fit_transform(X)\n",
    "\n",
    "# Vamos ver qual é a dimensão de W_nmf\n",
    "W_nmf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas se manteve em 58, que é o número de documentos (artigos) que nós temos, e o número de colunas se transformou em 6, que é o número de tópicos que nós escolhemos. Essa matriz gerada representa a matriz **W** (matriz de pesos) da fatoração de matrizes.\n",
    "\n",
    "Vamos agora achar a matriz **H** que representa a matriz de atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H_nmf = nmf.components_\n",
    "H_nmf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas é igual ao número de tópicos e o número de colunas representa o número de palavras no nosso vocabulário. Cada linha da matriz é definida como um componente (assim como o PCA possui os componentes principais) que está associado a um tópico específico. Entretanto, diferentemente do PCA, nós podemos associar cada componente a um conjunto específico de palavras. Vamos verificar abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precisamos criar uma lista de palavras que representam as \n",
    "# colunas da matriz de frequência de palavras\n",
    "words = [x[0] for x in sorted(tfidf.vocabulary_.items())]\n",
    "\n",
    "# Vamos criar um dataframe para visualizar\n",
    "components_df = pd.DataFrame(nmf.components_, columns=words)\n",
    "\n",
    "# Vamos verificar as palavras que representam cada um dos tópicos\n",
    "for i in range(6):\n",
    "    component = components_df.iloc[i]\n",
    "    print(\"Topico {}:\".format(i))\n",
    "    print(\"----------\")\n",
    "    print(component.nlargest(10))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou da distribuição de palavras dentro de cada tópico? Acha que faz sentido com os temas principais dos artigos do Wikipedia? Cada um dos tópicos poderia ser associado a um cluster?\n",
    "\n",
    "Podemos ainda verificar quais são os tópicos principais de alguns artigos específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Precisamos criar um dataframe para facilitar nossa vida\n",
    "df = pd.DataFrame(W_nmf, index=df_wiki['titulo'])\n",
    "\n",
    "examples = ['Denzel Washington','Leukemia','Neymar','LinkedIn','Arctic Monkeys','Global warming']\n",
    "\n",
    "for ex in examples:\n",
    "    print('Artigo: {}'.format(ex))\n",
    "    print('-'*30)\n",
    "    print('Topico\\tDist.\\tPalavras')\n",
    "    print('-'*30)\n",
    "    for i,topic in enumerate(df.loc[ex]):\n",
    "        words = components_df.iloc[i].nlargest(5).index.tolist()\n",
    "        print('{}\\t{:.2f}\\t{}'.format(i,topic,', '.join(words)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que os artigos possuem tópicos coerentes com o que esperávamos!\n",
    "\n",
    "Vamos agrupar agora os artigos pelos tópicos principais de cada um deles e ver como eles tém relação com os clusters definidos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cria as labels a partir do tópico mais relevante de cada artigo\n",
    "labels = np.argmax(W_nmf, axis=1)\n",
    "\n",
    "# Cria o novo dataframe com os labels dos clusters\n",
    "df = pd.DataFrame({'label': labels, 'article': df_wiki['titulo']})\n",
    "\n",
    "# Apresenta os resultados\n",
    "print(df.sort_values(by='label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Só por diversão, vamos verificar como ficaria um agrupamento usando Hierarchical Clustering na matriz de atributos obtida pelo NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "Y = linkage(W_nmf, method='ward', metric='euclidean')\n",
    "\n",
    "dendrogram(Y,\n",
    "           labels=df_wiki['titulo'].values,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou do resultado? Percebeu que o NMF não só encontrou uma representação em tópicos dos documentos, mas também teve um papel de agregador? Ele realizou um ótimo trabalho em encontrar clusters! E o melhor, nós podemos explicar com palavras o que representa cada um dos tópicos/clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Elo7: Motivos de Compra\n",
    "\n",
    "Agora aplique o NMF no dataset de motivos de compra e verifique o resultado comparado ao PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_reason.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.8, max_features=5000, sublinear_tf=True, use_idf=True)\n",
    "\n",
    "X = tfidf.fit_transform(df_reason['reason'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Encontre a matriz de pesos do NMF\n",
    "W_nmf = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Precisamos criar uma lista de palavras que representam as \n",
    "# colunas da matriz de frequência de palavras\n",
    "words = [x[0] for x in sorted(tfidf.vocabulary_.items())]\n",
    "\n",
    "# Vamos criar um dataframe para visualizar\n",
    "components_df = pd.DataFrame(nmf.components_, columns=words)\n",
    "\n",
    "# Vamos verificar as palavras que representam cada um dos tópicos\n",
    "for i in range(W_nmf.shape[1]):\n",
    "    component = components_df.iloc[i]\n",
    "    print(\"Topico {}:\".format(i))\n",
    "    print(\"----------\")\n",
    "    print(component.nlargest(10))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cria as labels a partir do tópico mais relevante de cada artigo\n",
    "labels = np.argmax(W_nmf, axis=1)\n",
    "\n",
    "# Cria o novo dataframe com os labels dos clusters\n",
    "df = pd.DataFrame({'label': labels, 'reason': df_reason['reason']})\n",
    "\n",
    "# Pega um sample\n",
    "df_sample = df.sample(n=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Vamos verificar a relação entre cada tópico encontrado\n",
    "# Para isso, vamos utilizar o hierarchical clustering\n",
    "X_sample = df_sample['reason'].values\n",
    "\n",
    "W_nmf = nmf.fit_transform(tfidf.fit_transform(X_sample))\n",
    "\n",
    "Y = linkage(W_nmf, method='ward', metric='euclidean')\n",
    "\n",
    "dendrogram(Y,\n",
    "           labels=df_sample['reason'].values,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Assim como o NMF, o [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) é um algoritmo relativamente simples (pelo menos algoritmicamente) e poderoso para conseguir encontrar tópicos dentro de documentos. Entretanto, diferentemente do NMF, o LDA se baseia em métodos probabilísticos. Mais especificamente, o **LDA** assume que cada **documento** foi gerado a partir de uma **mistura de tópicos** com uma distribuição de probabilidade (no caso, uma distribuição [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution)), e cada **tópico** foi gerado por uma distribuição de **palavras** com certa probabilidade associada (distribuição [multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution)).\n",
    "\n",
    "Para imaginar um cenário, imagine que temos os seguintes documentos (retirados deste [link](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)):\n",
    "1. I ate a banana and spinach smoothie for breakfast\n",
    "2. I like to eat broccoli and bananas.\n",
    "3. Chinchillas and kittens are cute.\n",
    "4. My sister adopted a kitten yesterday.\n",
    "5. Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "Para o LDA, os documentos poderiam ser formados pelos seguintes tópicos e palavras:\n",
    "- Documentos 1 e 2: 100% Tópico A\n",
    "- Documentos 3 e 4: 100% Tópico B\n",
    "- Documento 5: 60% Tópico A, 40% Tópico B\n",
    "\n",
    "- Tópico A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching (Talvez represente algo relacionado a comida)\n",
    "- Tópico B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster (Talvez seja relacionado a animais fofinhos)\n",
    "\n",
    "Esse comportamento do LDA é interessante, já que temos uma representação mais intuitiva da formação dos documentos. Poderíamos até criar documentos artificiais a partir dessas definições.\n",
    "\n",
    "Agora vamos ver um exemplo de aplicação utilizando o dataset do Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv(os.path.join(DATASET_FOLDER, 'wikipedia_dataset.csv'), sep=';')\n",
    "\n",
    "df_wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O LDA não pode ser utilizado com Tf-Idf, porque ele precisa da contagem total de palavras nos documentos. Por esse motivo, vamos utilizar o método [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do scikit-learn. Esse método apenas realiza a contagem de frequência de palavras nos documentos. Para evitar que \"stopwords\" tenham uma importância indevida, podemos remove-las do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importa o módulo CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf = CountVectorizer(max_df=0.95, min_df=0.01, max_features=15000, stop_words='english')\n",
    "\n",
    "X = tf.fit_transform(df_wiki['artigo'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Agora vamos importar o módulo LDA do scikit-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Precisamos criar a instância do LDA\n",
    "# Temos que definir um número de tópicos do LDA\n",
    "# Como temos 6 clusters, vamos escolher n_topics=6\n",
    "lda = LatentDirichletAllocation(n_topics=6, learning_method='batch')\n",
    "\n",
    "# Agora vamos utilizar os mesmos atributos fit, transform ou fit_transform\n",
    "# que já conhecemos do universo do sklearn\n",
    "X_lda = lda.fit_transform(X)\n",
    "\n",
    "# Vamos ver qual é a dimensão de X_lda\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas se manteve em 58, que é o número de documentos (artigos) que nós temos, e o número de colunas se transformou em 6, que é o número de tópicos que nós escolhemos. \n",
    "\n",
    "Vamos agora achar a matriz de atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normaliza os componentes\n",
    "lda_components = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "print(lda_components.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas é igual ao número de tópicos e o número de colunas representa o número de palavras no nosso vocabulário. Essa matriz é semelhante a matriz de atributos do NMF.\n",
    "\n",
    "Vamos achar quais são as palavras mais importantes para cada tópico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precisamos criar uma lista de palavras que representam as \n",
    "# colunas da matriz de frequência de palavras\n",
    "words = [x[0] for x in sorted(tf.vocabulary_.items())]\n",
    "\n",
    "# Vamos criar um dataframe para visualizar\n",
    "components_df = pd.DataFrame(lda_components, columns=words)\n",
    "\n",
    "# Vamos verificar as palavras que representam cada um dos tópicos\n",
    "for i in range(6):\n",
    "    component = components_df.iloc[i]\n",
    "    print(\"Topico {}:\".format(i))\n",
    "    print(\"----------\")\n",
    "    print(component.nlargest())\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou da distribuição de palavras dentro de cada tópico? Acha que faz sentido com os temas principais dos artigos do Wikipedia? Cada um dos tópicos poderia ser associado a um cluster?\n",
    "\n",
    "Podemos ainda verificar quais são os tópicos principais de alguns artigos específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precisamos criar um dataframe para facilitar nossa vida\n",
    "df = pd.DataFrame(X_lda, index=df_wiki['titulo'])\n",
    "\n",
    "examples = ['Denzel Washington','Leukemia','Neymar','LinkedIn','Arctic Monkeys','Global warming']\n",
    "\n",
    "for ex in examples:\n",
    "    print('Artigo: {}'.format(ex))\n",
    "    print('-'*30)\n",
    "    print('Topico\\tDist.\\tPalavras')\n",
    "    print('-'*30)\n",
    "    for i,topic in enumerate(df.loc[ex]):\n",
    "        words = components_df.iloc[i].nlargest(5).index.tolist()\n",
    "        print('{}\\t{:.2f}\\t{}'.format(i,topic,', '.join(words)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cria as labels a partir do tópico mais relevante de cada artigo\n",
    "labels = np.argmax(X_lda, axis=1)\n",
    "\n",
    "# Cria o novo dataframe com os labels dos clusters\n",
    "df = pd.DataFrame({'label': labels, 'titulo': df_wiki['titulo']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = linkage(X_lda, method='ward', metric='euclidean')\n",
    "\n",
    "dendrogram(Y,\n",
    "           labels=df_wiki['titulo'].values,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=14,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Elo7 - Subcategorias Automáticas\n",
    "\n",
    "Vamos para mais um case real do Elo7!\n",
    "\n",
    "Esse case é um dos trabalhos mais recentes do time de Data Science do Elo7. De fato, é um trabalho ainda em aberto e qualquer sugestão de melhorias é bem vinda! =)\n",
    "\n",
    "- O problema:\n",
    "O Elo7 possui uma árvore de categorias dividida em N1 e N2. O primeiro nível (N1) contém as categorias \"alto nível\" do site. São as categorias mais genéricas do marketplace- ou, pelo menos, é assim gostaríamos que fosse. As categorias N2, ou subcategorias, são as possíveis extensões dos nós das categorias N1. Podemos perceber que a árvore é extremamente limitada e isso é um problema grave não só para os compradores, que não conseguem navegar nas nossas categorias, mas também para os vendedores, que não conseguem categorizar bem seus produtos. A solução para esse problema seria uma árvore de categorias com maior \"granularidade\", ou seja, que consiga expandir além dos 2 níveis e ter mais subcategorias.\n",
    "\n",
    "- O que o time de Data Science tem a ver com essa história? \n",
    "\n",
    "Bom, gerar uma nova árvore de categoria pode ser uma tarefa bastante monótona e cansativa. Provavelmente deve haver algum jeito de encontrar bons agrupamentos de produtos que pudessem servir como uma nova subcategoria. Talvez algum método de clustering que utilize como features o conteúdo dos produtos pode gerar algum resultado interessante.\n",
    "\n",
    "- O experimento:\n",
    "\n",
    "O dataset a seguir possui um subconjunto de produtos que foram categorizados na categoria N1 \"Casamento\". Escolhemos esse conjunto de dados para iniciar nossos trabalhos, porque assim temos mais controle sobre nossos resultados. E, também, porque é uma das categorias mais importantes do marketplace.\n",
    "\n",
    "Para essa tarefa, vamos utilizar apenas o título e uma parte da descrição do produto (aprox. 140 caracteres) como features de entrada.\n",
    "\n",
    "Vamos analisar os dados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cat = pd.read_csv(os.path.join(DATASET_FOLDER, 'subcategory_elo7_dataset.csv'), sep=';')\n",
    "\n",
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma coluna com as features que vamos incluir no nosso modelo de aprendizagem.\n",
    "Esse vetor de features será o título + descrição do produto. Para compensar a quantidade de palavras do título em relação a descrição, vamos repetir o título duas vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cat['title_desc'] = (df_cat['title'] + ' ')*2 + df_cat['short_description']\n",
    "\n",
    "df_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar nossa matriz de features, nós vamos utilizar o Tf-Idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.9, max_features=10000, sublinear_tf=True, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df_cat['title_desc'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Agora aplique os métodos encontrados até agora para tentar encontrar uma boa árvore de categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Sistema de Recomendação\n",
    "\n",
    "Esse case foi retirado desse [artigo](https://towardsdatascience.com/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac). O objetivo dele é verificar se conseguimos extrair tópicos relevantes de um dataset contendo 8.447 matérias do NY Times, com um vocabulário de 3.012 palavras. Por razão de direitos autorias, os documentos não possuem título.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1400/1*toWf7lAVf_5GIb9IMfS8Bw.jpeg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O arquivo `nyt_dataset.csv` contém os documentos, onde cada linha representa um documento específico e cada coluna representa a frequência de cada palavra. O dataset já sofreu um processo de limpeza e vetorização, onde foram mantidas apenas palavras que ocorreram mais de 10 vezes.\n",
    "\n",
    "Vamos criar a matriz de frequências a partir desses dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_nyt = pd.read_csv(os.path.join(DATASET_FOLDER, 'nyt_dataset.csv'), sep=';')\n",
    "\n",
    "df_nyt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_nyt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Encontrar a matriz de pesos do NMF\n",
    "# Dica: escolher entre 20 e 25 tópicos\n",
    "# Teste com outros números e verifique \n",
    "# a qualidade\n",
    "\n",
    "W_nmf = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas se manteve em 8447, que é o número de documentos (artigos) que nós temos, e o número de colunas se transformou em 25, que é o número de tópicos que nós escolhemos. Essa matriz gerada representa a matriz **W** (matriz de pesos) da fatoração de matrizes.\n",
    "\n",
    "Vamos agora achar a matriz **H** que representa a matriz de atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi possível verificar que o número de linhas é igual ao número de tópicos e o número de colunas representa o número de palavras no nosso vocabulário? Cada linha da matriz é definida como um componente (assim como o PCA possui os componentes principais) que está associado a um tópico específico. Entretanto, diferentemente do PCA, nós podemos associar cada componente a um conjunto específico de palavras. Vamos verificar abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Verifique a distribuição de palavras em cada topico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sistemas de recomendação\n",
    "\n",
    "Em poucos anos a internet revolucionou o mercado de consumo mundial e a forma como os clientes interagem com os vendedores. Uma das dinâmicas criadas mais importantes a partir dessa revolução é a de sistemas de recomendação. Todos devem já ter notado o quanto e-commerces como a Amazon, Best Buy e até empresas brasileiras acertam ao recomendar certos tipos de produto para seus clientes, que muitas vezes nem os estavam procurando (veja esse [artigo](https://www.techemergence.com/use-cases-recommendation-systems/)). Isso não acontece mais apenas em e-commerces, mas também com provedores de música (Spotify), de filmes (Netflix) ou vídeos em geral (Youtube). \n",
    "\n",
    "Esse fenômeno só ocorre devido a um fator: **dados**. As empresas atualmente possuem muita informação sobre os seus produtos e os seus usuários. É muito fácil obter, hoje em dia, os interesses dos clientes sobre seus produtos. Seja o fato de o vendedor comprar um produto, dar um review ou apenas clicar, já é suficiente para uma empresa mapear os interesses dos usuários e tentar direcionar produtos que seriam relevantes para o usuário sem nem mesmo ele saber!\n",
    "\n",
    "Os sistemas de recomendação se baseiam, basicamente, em encontrar relações entre compradores e produtos. Mais especificamente, existem dois grandes grupos de sistemas de recomendação:\n",
    "- **Proximidade de produtos**: Tem o objetivo de encontrar produtos similares aos consumidos por um cliente. Se um cliente possui o interesse em um determinado produto, o sistema de recomendação pode tentar encontrar outros produtos similares para indicar para o cliente.\n",
    "- **Proximidade entre clientes** (Filtro Colaborativo): Tem o objetivo de encontrar clientes com interesses semelhantes. Suponha que exista um cliente X que consuma os produtos A e B, enquanto um outro cliente Y tem interesse nos produtos A, B e C. Como eles possuem interesses semelhantes (produtos A e B), o sistema de recomendação poderia indicar o produto C para o cliente X.\n",
    "\n",
    "Existem diversos outros tipos de sistemas de recomendação que fogem do escopo desse material. Mais informações podem ser vistas nesse [link](https://www.techemergence.com/use-cases-recommendation-systems/).\n",
    "\n",
    "##### Exemplo\n",
    "Agora vamos tentar criar um sistema simples de recomendação baseado em **proximidade de produtos**! \n",
    "\n",
    "Vamos utilizar o dataset de artigos do NY Times para essa tarefa. Vamos supor que uma pessoa tenha lido um determinado artigo e nós gostaríamos de recomendar outros artigos semelhantes àquele. Lembre que para comparar dois documentos que contém vetores de atributos associados a palavras, a melhor medida de distância é a **distância de cossenos**.\n",
    "\n",
    "Para calcular a distância entre produtos, nós poderíamos utilizar todo o espaço de atributos (quantidade de palavras no vocabulário), mas isso é desnecessário. Nós temos uma representação resumida de cada documento por sua proporção de tópicos, que formam o novo vetor de atributos. Como já encontramos os tópicos relacionados aos documentos no exercício anterior, podemos aproveitá-los para resolver nosso problema atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nós vamos ter que normalizar o vetor de atributos\n",
    "# Isso é necessário para que todas as features (tópicos) \n",
    "# de um documento some 1 ao final\n",
    "# Seria a porcentagem de composição dos tópicos \n",
    "# no documento\n",
    "from sklearn.preprocessing import normalize\n",
    "W_nmf_norm = normalize(W_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos criar um dataframe:\n",
    "# índice: documento\n",
    "# colunas: vetor de features (normalizadas)\n",
    "df = pd.DataFrame(W_nmf_norm,\n",
    "                  columns=['topic {}'.format(i) for i in range(n_topics)],\n",
    "                  index=['doc {}'.format(i) for i in range(len(df_nyt))])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vamos escolher um documento aleatoriamente\n",
    "# Esse documento será o escolhido pelo cliente\n",
    "doc_target = np.random.choice(range(len(df_nyt)))\n",
    "doc_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verifica o vetor de features dele:\n",
    "doc_target_features = df.iloc[doc_target]\n",
    "print(doc_target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calcula a distância de cossenos entre o artigo e \n",
    "# todos os outros documentos\n",
    "cossine_distance = df.dot(doc_target_features)\n",
    "\n",
    "# Todos os documentos com maior distância de cossenos \n",
    "# representam os documentos mais próximos\n",
    "print(\"Artigos recomendados:\")\n",
    "recommendations = cossine_distance.nlargest(6)[1:]\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default_top_topics = df.iloc[doc_target].nlargest(3).index.values\n",
    "default_components = [components_df.iloc[int(i.split(' ')[1])].nlargest(5).index.values \n",
    "                      for i in default_top_topics]\n",
    "\n",
    "print(\"Artigo original: doc {}\".format(doc_target))\n",
    "print(\"-----------------------\")\n",
    "print(\"- Tópicos (palavras): \")\n",
    "for i in range(len(default_top_topics)):\n",
    "    print(\"{:>15} ({})\".format(default_top_topics[i], ', '.join(default_components[i])))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "for doc, similarity in recommendations.items():\n",
    "    doc_num = int(doc.split(' ')[1])\n",
    "    top_topics = df.iloc[doc_num].nlargest(3).index.values\n",
    "    components = [components_df.iloc[int(i.split(' ')[1])].nlargest(5).index.values for i in top_topics]\n",
    "    print(\"Recomendação {}:\".format(doc))\n",
    "    print(\"-----------------------\")\n",
    "    print(\"- Similaridade: {:.2%}\".format(similarity))\n",
    "    print(\"- Tópicos (palavras): \")\n",
    "    for i in range(len(top_topics)):\n",
    "        print(\"{:>15} ({})\".format(top_topics[i], ', '.join(components[i])))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
